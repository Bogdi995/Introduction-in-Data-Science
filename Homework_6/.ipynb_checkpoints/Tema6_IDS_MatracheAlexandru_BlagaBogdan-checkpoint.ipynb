{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea587ec4",
   "metadata": {},
   "source": [
    "# Tema 6\n",
    "\n",
    "Folosind un set de date - de exemplu de la https://archive.ics.uci.edu/ml/datasets.php?format=&task=&att=&area=&numAtt=&numIns=&type=text&sort=taskDown&view=table - sa se rezolve o problema de clasificare sau regresie, plecand de la intrari de tip text.\n",
    "\n",
    "Se poate opta pentru codificare BOW, n-grams, word2vec sau altele adecvate. Modelele de predictie pot fi din biblioteca scikit-learn. Puteti folosi pentru preprocesare biblioteca [NLTK](https://www.nltk.org).\n",
    "\n",
    "Pentru clasificare se va optimiza scorul F1; se vor raporta scorurile F1 si acuratetea. Pentru regresie se va optimiza scorul mean squared error; se vor raporta scorurile MSE, mean absolute error, r2.\n",
    "\n",
    "Exemple:\n",
    "1. [Clasificare de SMS-uri](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection)\n",
    "1. [Sentence Classification Data Set](https://archive.ics.uci.edu/ml/datasets/Sentence+Classification#)\n",
    "1. [Sentiment Labelled Sentences Data Set](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences)\n",
    "1. [Victorian Era Authorship Attribution Data Set](https://archive.ics.uci.edu/ml/datasets/Victorian+Era+Authorship+Attribution)\n",
    "1. [Amazon Commerce reviews set Data Set](https://archive.ics.uci.edu/ml/datasets/Amazon+Commerce+reviews+set)\n",
    "1. [Farm Ads Data Set](https://archive.ics.uci.edu/ml/datasets/Farm+Ads)\n",
    "\n",
    "\n",
    "Se vor investiga minim 2 seturi de date si pentru fiecare din ele minim 4 modele de clasificare sau de regresie. Daca setul de date e deja impartit in train si test, se va folosi ca atare - setul de antrenare se va imparti, suplimentar in train + validation; altfel, se va face  5 fold CV. Valorile optime ale hiperparametrilor vor fi alese prin random search sau grid search.\n",
    "\n",
    "Pentru fiecare set de date:\n",
    "1. (2 x 0.5 p) Se descrie setul de date, in limba romana (continut, provenienta, problema etc.)\n",
    "1. (2 x 1 p) Se face analiza exploratorie, folosind cod Python: distributia claselor sau a valorilor continue de iesire - numeric si grafic, statistici asupra textelor (de exemplu: lungime minima/medie/maxima; cele mai frecvente k cuvinte; clustering etc.). Se va explica fiecare pas si ce se urmareste prin efectuarea lui. Graficele vor avea axele numite (ce se reprezinta, evetual unitate de masura)\n",
    "1. (2 x 0.5 p) Se face preprocesare de date; se explica in limba romana care sunt metodele de preprocesare folosite, efectul lor pe datele de intrare, ce forma are iesirea obtinuta; se arata efectele pasilor de preprocesare asupra setului de date (noul numar de documente, dinamica vocabularului, trasaturile rezultate etc.) Se pot aduga grafice si tabele la acest pas.\n",
    "1. (2 x 4 x 0.5 p) Clasificare sau regresie, dupa caz: se face o descriere a modelelor considerate, in limba romana; se descrie modalitatea de cautare a hiperparametrilor; rezultatele obtinute se vor prezenta tabelar, similar cu tema precedenta. \n",
    "\n",
    "Se acorda doua puncte din oficiu.\n",
    "\n",
    "Descrierea modelelor si a pasilor de preprocesare pot fi in sectiuni separate, cu referinte la acestea unde e necesar.\n",
    "\n",
    "Exemple:\n",
    "1. [Working With Text Data](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)\n",
    "1. [Text Classification with Python and Scikit-Learn](https://stackabuse.com/text-classification-with-python-and-scikit-learn/)\n",
    "1. [How to Prepare Text Data for Machine Learning with scikit-learn](https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a579d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\bogdi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bogdi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import scipy\n",
    "import pandas\n",
    "import sklearn\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from typing import List, Dict, Union\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_validate\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efe498d",
   "metadata": {},
   "source": [
    "### Amazon Fine Foods Reviews\n",
    "##### Sursa:\n",
    "J. McAuley și J. Leskovec. De la amatori la cunoscători: modelarea evoluției expertizei utilizatorilor prin recenzii online. WWW, 2013.\n",
    "\n",
    "##### Informatii set de date:\n",
    "Acest set de date constă în recenzii despre mâncăruri fine de pe Amazon. Datele se întind pe o perioadă de mai mult de 10 ani, incluzând toate cele ~ 500.000 de recenzii până în octombrie 2012. Recenziile includ informații despre produse și utilizatori, evaluări și o recenzie în text clar. De asemenea, avem recenzii din toate celelalte categorii Amazon.\n",
    "\n",
    "##### Informatii despre atribute:\n",
    "Număr de recenzii: 568.454 <br>\n",
    "Număr de utilizatori: 256.059 <br>\n",
    "Număr de produse: 74.258 <br>\n",
    "Utilizatori cu > 50 recenzii: 260 <br>\n",
    "Mediană nr. de cuvinte pe recenzie: 56 <br>\n",
    "Durata timp: Oct 1999 - Oct 2012 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41fbb61",
   "metadata": {},
   "source": [
    "### Corona Tweets Dateset\n",
    "##### Sursa:\n",
    "Necunoscuta\n",
    "\n",
    "##### Informatii set de date:\n",
    "Acest det de date are tweet-urile extrase de pe Twitter, iar etichetarea lor a fost facuta manual.\n",
    "Numele și numele de utilizator au primit coduri pentru a evita orice probleme de confidențialitate.\n",
    "\n",
    "##### Informatii despre atribute:\n",
    "Coloane:\n",
    "1) Location <br>\n",
    "2) Tweet At <br>\n",
    "3) Original Tweet <br>\n",
    "4) Label <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "864fe967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataframe shape is: (568401, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ProductId          UserId                      ProfileName   \n",
       "Id                                                                \n",
       "1   B001E4KFG0  A3SGXH7AUHU8GW                       delmartian  \\\n",
       "2   B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "3   B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "4   B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "5   B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "    HelpfulnessNumerator  HelpfulnessDenominator  Score        Time   \n",
       "Id                                                                    \n",
       "1                      1                       1      5  1303862400  \\\n",
       "2                      0                       0      1  1346976000   \n",
       "3                      1                       1      4  1219017600   \n",
       "4                      3                       3      2  1307923200   \n",
       "5                      0                       0      5  1350777600   \n",
       "\n",
       "                  Summary                                               Text  \n",
       "Id                                                                            \n",
       "1   Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "2       Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "3   \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "4          Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "5             Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df:object = pd.read_csv('./data/Reviews.csv', index_col=0)\n",
    "df = df.dropna()\n",
    "print(f'The dataframe shape is: {df.shape}')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25a40c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataframe shape after removing the duplicate rows is: (393914, 9)\n"
     ]
    }
   ],
   "source": [
    "df.drop_duplicates(subset={'UserId', 'ProfileName', 'Time', 'Text'}, inplace=True)\n",
    "print(f'The dataframe shape after removing the duplicate rows is: {df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faca855a",
   "metadata": {},
   "source": [
    "Grafic care ne arata care review-uri date de clienti sunt majoritare. <br>\n",
    "Se observa predominanta celor cu scorul cel mai mare (aprox. 250000). <br>\n",
    "Putem imparti review-urile in categorii in functie de scorul acestora dupa cum urmeaza: <br>\n",
    "Scor > 3 -> review pozitiv <br>\n",
    "Scor = 3 -> review neutru <br>\n",
    "Scor < 3 -> review negativ <br>\n",
    "Din nou, se observa predominanta celor pozitive (aprox. 310000), totusi, nici cele negative nu sunt o cantitate neglijabila (aprox. 70000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "470a7180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax:object = df['Score'].value_counts().plot(kind='bar', figsize=(7,7))\n",
    "fig:object = ax.get_figure()\n",
    "ax.set_title(\"Amazon's Fine Food Reviews Dataset\")\n",
    "ax.set_xlabel('Score')\n",
    "ax.set_ylabel('Value Counts');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc25cfcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A concise summary of the DataFrame is:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 393914 entries, 1 to 568454\n",
      "Data columns (total 9 columns):\n",
      " #   Column                  Non-Null Count   Dtype \n",
      "---  ------                  --------------   ----- \n",
      " 0   ProductId               393914 non-null  object\n",
      " 1   UserId                  393914 non-null  object\n",
      " 2   ProfileName             393914 non-null  object\n",
      " 3   HelpfulnessNumerator    393914 non-null  int64 \n",
      " 4   HelpfulnessDenominator  393914 non-null  int64 \n",
      " 5   Score                   393914 non-null  int64 \n",
      " 6   Time                    393914 non-null  int64 \n",
      " 7   Summary                 393914 non-null  object\n",
      " 8   Text                    393914 non-null  object\n",
      "dtypes: int64(4), object(5)\n",
      "memory usage: 30.1+ MB\n"
     ]
    }
   ],
   "source": [
    "print('A concise summary of the DataFrame is:')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542f6843",
   "metadata": {},
   "source": [
    "Dorim sa aflam si alte informatii despre review-uri, cum ar fi: <br>\n",
    "Cel mai lung, cel mai scurt review <br>\n",
    "Cel mai lung, cel mai scurt rezumat <br>\n",
    "Persoana cu cele mai multe review-uri <br>\n",
    "Aceste date ne ajuta sa avem o mai buna privire de ansamblu asupra setului de date cu care lucram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dba75383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest review written is: 21409\n",
      "The shortest review written is: 12\n",
      "The longest summary written is: 128\n",
      "The shortest summary written is: 1\n",
      "The name of the profile with the most reviews is:\n",
      "0    Gary Peterson\n",
      "Name: ProfileName, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(f'The longest review written is: {df.Text.map(lambda x: len(x)).max()}')\n",
    "print(f'The shortest review written is: {df.Text.map(lambda x: len(x)).min()}')\n",
    "print(f'The longest summary written is: {df.Summary.map(lambda x: len(x)).max()}')\n",
    "print(f'The shortest summary written is: {df.Summary.map(lambda x: len(x)).min()}')\n",
    "print(f'The name of the profile with the most reviews is:')\n",
    "print(df.ProfileName.mode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c3c4dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions:Dict[str,str] = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61c10f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text:str, remove_stopwords:bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings\n",
    "    param text: the text that is going to be formated\n",
    "    param remove_stopwords: \n",
    "    return:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Converteste cuvintele astfel incat sa aiba doar litere mici\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Inlocuieste contractiile cu forma lor lunga\n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text:List[str] = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "    \n",
    "    # Formateaza cuvintele si inlatura caracterele nedorite cum ar fi: ghilimele, puncte, etc\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    \n",
    "    # inlatura cuvintele nefolositoare\n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops:set = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    # Tokenizarea textului: se impart propozitiile in cuvinte\n",
    "    text =  nltk.WordPunctTokenizer().tokenize(text)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "909638d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text_Cleaned'] = list(map(clean_text, df.Text))\n",
    "def lemmatized_words(text):\n",
    "    lemm:nltk.stem.wordnet.WordNetLemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    df['Lemmatized_text'] = list(map(lambda word:list(map(lemm.lemmatize, word)),df.Text_Cleaned))\n",
    "\n",
    "lemmatized_words(df.Text_Cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1034271e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>Text_Cleaned</th>\n",
       "      <th>Lemmatized_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>[bought, several, vitality, canned, dog, food,...</td>\n",
       "      <td>[bought, several, vitality, canned, dog, food,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>[product, arrived, labeled, jumbo, salted, pea...</td>\n",
       "      <td>[product, arrived, labeled, jumbo, salted, pea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>[confection, around, centuries, light, pillowy...</td>\n",
       "      <td>[confection, around, century, light, pillowy, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>[looking, secret, ingredient, robitussin, beli...</td>\n",
       "      <td>[looking, secret, ingredient, robitussin, beli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>[great, taffy, great, price, wide, assortment,...</td>\n",
       "      <td>[great, taffy, great, price, wide, assortment,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ProductId          UserId                      ProfileName   \n",
       "Id                                                                \n",
       "1   B001E4KFG0  A3SGXH7AUHU8GW                       delmartian  \\\n",
       "2   B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "3   B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "4   B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "5   B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "    HelpfulnessNumerator  HelpfulnessDenominator  Score        Time   \n",
       "Id                                                                    \n",
       "1                      1                       1      5  1303862400  \\\n",
       "2                      0                       0      1  1346976000   \n",
       "3                      1                       1      4  1219017600   \n",
       "4                      3                       3      2  1307923200   \n",
       "5                      0                       0      5  1350777600   \n",
       "\n",
       "                  Summary                                               Text   \n",
       "Id                                                                             \n",
       "1   Good Quality Dog Food  I have bought several of the Vitality canned d...  \\\n",
       "2       Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...   \n",
       "3   \"Delight\" says it all  This is a confection that has been around a fe...   \n",
       "4          Cough Medicine  If you are looking for the secret ingredient i...   \n",
       "5             Great taffy  Great taffy at a great price.  There was a wid...   \n",
       "\n",
       "                                         Text_Cleaned   \n",
       "Id                                                      \n",
       "1   [bought, several, vitality, canned, dog, food,...  \\\n",
       "2   [product, arrived, labeled, jumbo, salted, pea...   \n",
       "3   [confection, around, centuries, light, pillowy...   \n",
       "4   [looking, secret, ingredient, robitussin, beli...   \n",
       "5   [great, taffy, great, price, wide, assortment,...   \n",
       "\n",
       "                                      Lemmatized_text  \n",
       "Id                                                     \n",
       "1   [bought, several, vitality, canned, dog, food,...  \n",
       "2   [product, arrived, labeled, jumbo, salted, pea...  \n",
       "3   [confection, around, century, light, pillowy, ...  \n",
       "4   [looking, secret, ingredient, robitussin, beli...  \n",
       "5   [great, taffy, great, price, wide, assortment,...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a1ed2ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "      <th>Text_Cleaned</th>\n",
       "      <th>Lemmatized_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90030</th>\n",
       "      <td>2</td>\n",
       "      <td>They come in packages that are folded, not sealed shut, and the quality was uneven.  Perhaps 20% of the pears are truly soft and moist, the rest are varying shades of dried out.  Will not be buying these again.&lt;br /&gt;&lt;br /&gt;I received some dried pears as a present years ago and those pears were uniformly big, moist and juicy!  I wish I remembered the brand, cause good dried pears are wonderful!</td>\n",
       "      <td>[come, packages, folded, sealed, shut, quality, uneven, perhaps, 20, pears, truly, soft, moist, rest, varying, shades, dried, buying, &lt;, br, &gt;&lt;, br, &gt;, i, received, dried, pears, present, years, ago, pears, uniformly, big, moist, juicy, wish, remembered, brand, cause, good, dried, pears, wonderful]</td>\n",
       "      <td>[come, package, folded, sealed, shut, quality, uneven, perhaps, 20, pear, truly, soft, moist, rest, varying, shade, dried, buying, &lt;, br, &gt;&lt;, br, &gt;, i, received, dried, pear, present, year, ago, pear, uniformly, big, moist, juicy, wish, remembered, brand, cause, good, dried, pear, wonderful]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84578</th>\n",
       "      <td>5</td>\n",
       "      <td>This hot sauce is one of a kind. I am always hesitant to try spicy foods because of the taste of hot sauce.. but when I tried Rocky's I was transformed. Other hot sauces seem chemically to me but Rocky's has a homey flavor that is incredibly diverse-- so much that I have been using it on eggs, tacos, as a marinade for chicken and on grilled cheese!! My family and I have been through six bottles of Rockys in the last two months. I can honestly say this is a first for our family, but certainly...</td>\n",
       "      <td>[hot, sauce, one, kind, always, hesitant, try, spicy, foods, taste, hot, sauce, tried, rocky, transformed, hot, sauces, seem, chemically, rocky, homey, flavor, incredibly, diverse, much, using, eggs, tacos, marinade, chicken, grilled, cheese, family, six, bottles, rockys, last, two, months, honestly, say, first, family, certainly, last, rocky, become, staple, table, every, meal, thank, creating, amazing, hot, sauce, tried, order, &lt;, br, &gt;&lt;, br, &gt;, lime, notes, zingy, fresh, lime, awful, conc...</td>\n",
       "      <td>[hot, sauce, one, kind, always, hesitant, try, spicy, food, taste, hot, sauce, tried, rocky, transformed, hot, sauce, seem, chemically, rocky, homey, flavor, incredibly, diverse, much, using, egg, taco, marinade, chicken, grilled, cheese, family, six, bottle, rockys, last, two, month, honestly, say, first, family, certainly, last, rocky, become, staple, table, every, meal, thank, creating, amazing, hot, sauce, tried, order, &lt;, br, &gt;&lt;, br, &gt;, lime, note, zingy, fresh, lime, awful, concentrate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390282</th>\n",
       "      <td>3</td>\n",
       "      <td>There's nothing like a Coca-Cola with REAL sugar.  I live for Passover time when I can get Kosher Coke at the local supermarket, which also has real sugar.  For the in between times, I like to grab some Mexi-Coke.  It's so much better than the regular Coke that is readily available at my local supermarket.  If I didn't have a place close to my home where I could get it, I might consider paying the $21 here on Amazon.  But since I can go to Wegman's and get a whole case (24 bottles) for $1 LE...</td>\n",
       "      <td>[nothing, like, coca, cola, real, sugar, live, passover, time, get, kosher, coke, local, supermarket, also, real, sugar, times, like, grab, mexi, coke, much, better, regular, coke, readily, available, local, supermarket, place, close, home, could, get, might, consider, paying, 21, amazon, since, go, wegman, get, whole, case, 24, bottles, 1, less, costs, 6, bottles, pass, 5, star, product, 1, star, price, split, difference, 3, stars, review, wegman, near, consider, taking, advantage, offer, w...</td>\n",
       "      <td>[nothing, like, coca, cola, real, sugar, live, passover, time, get, kosher, coke, local, supermarket, also, real, sugar, time, like, grab, mexi, coke, much, better, regular, coke, readily, available, local, supermarket, place, close, home, could, get, might, consider, paying, 21, amazon, since, go, wegman, get, whole, case, 24, bottle, 1, le, cost, 6, bottle, pas, 5, star, product, 1, star, price, split, difference, 3, star, review, wegman, near, consider, taking, advantage, offer, wegman, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125128</th>\n",
       "      <td>1</td>\n",
       "      <td>I remember the first time I tried a Lindt 70% bar, when I visited France as a child.  My parents bought several to bring back to the States, and they were cherished and eaten slowly.  I've enjoyed them as a special treat ever since.&lt;br /&gt;&lt;br /&gt;Sadly, that era has ended.  The \"new recipe\" is a shadow of the former Lindt chocolate.  It tastes like pretty much any other standard dark chocolate bar -- none of the crisp, vibrant, complexity it used to have.&lt;br /&gt;&lt;br /&gt;I guess all good things even...</td>\n",
       "      <td>[remember, first, time, tried, lindt, 70, bar, visited, france, child, parents, bought, several, bring, back, states, cherished, eaten, slowly, enjoyed, special, treat, ever, since, &lt;, br, &gt;&lt;, br, &gt;, sadly, era, ended, new, recipe, shadow, former, lindt, chocolate, tastes, like, pretty, much, standard, dark, chocolate, bar, none, crisp, vibrant, complexity, used, &lt;, br, &gt;&lt;, br, &gt;, i, guess, good, things, eventually, come, end, bad]</td>\n",
       "      <td>[remember, first, time, tried, lindt, 70, bar, visited, france, child, parent, bought, several, bring, back, state, cherished, eaten, slowly, enjoyed, special, treat, ever, since, &lt;, br, &gt;&lt;, br, &gt;, sadly, era, ended, new, recipe, shadow, former, lindt, chocolate, taste, like, pretty, much, standard, dark, chocolate, bar, none, crisp, vibrant, complexity, used, &lt;, br, &gt;&lt;, br, &gt;, i, guess, good, thing, eventually, come, end, bad]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90827</th>\n",
       "      <td>1</td>\n",
       "      <td>Reese eggs taste great and are one of my favorite candies. But these were a melted mess.  Changes texture and taste. Price was good but not worth it.  Also could not return.  Booooooooooo....</td>\n",
       "      <td>[reese, eggs, taste, great, one, favorite, candies, melted, mess, changes, texture, taste, price, good, worth, also, could, return, booooooooooo]</td>\n",
       "      <td>[reese, egg, taste, great, one, favorite, candy, melted, mess, change, texture, taste, price, good, worth, also, could, return, booooooooooo]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Score   \n",
       "Id              \n",
       "90030       2  \\\n",
       "84578       5   \n",
       "390282      3   \n",
       "125128      1   \n",
       "90827       1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Text   \n",
       "Id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "90030                                                                                                           They come in packages that are folded, not sealed shut, and the quality was uneven.  Perhaps 20% of the pears are truly soft and moist, the rest are varying shades of dried out.  Will not be buying these again.<br /><br />I received some dried pears as a present years ago and those pears were uniformly big, moist and juicy!  I wish I remembered the brand, cause good dried pears are wonderful!  \\\n",
       "84578   This hot sauce is one of a kind. I am always hesitant to try spicy foods because of the taste of hot sauce.. but when I tried Rocky's I was transformed. Other hot sauces seem chemically to me but Rocky's has a homey flavor that is incredibly diverse-- so much that I have been using it on eggs, tacos, as a marinade for chicken and on grilled cheese!! My family and I have been through six bottles of Rockys in the last two months. I can honestly say this is a first for our family, but certainly...   \n",
       "390282  There's nothing like a Coca-Cola with REAL sugar.  I live for Passover time when I can get Kosher Coke at the local supermarket, which also has real sugar.  For the in between times, I like to grab some Mexi-Coke.  It's so much better than the regular Coke that is readily available at my local supermarket.  If I didn't have a place close to my home where I could get it, I might consider paying the $21 here on Amazon.  But since I can go to Wegman's and get a whole case (24 bottles) for $1 LE...   \n",
       "125128  I remember the first time I tried a Lindt 70% bar, when I visited France as a child.  My parents bought several to bring back to the States, and they were cherished and eaten slowly.  I've enjoyed them as a special treat ever since.<br /><br />Sadly, that era has ended.  The \"new recipe\" is a shadow of the former Lindt chocolate.  It tastes like pretty much any other standard dark chocolate bar -- none of the crisp, vibrant, complexity it used to have.<br /><br />I guess all good things even...   \n",
       "90827                                                                                                                                                                                                                                                                                                                       Reese eggs taste great and are one of my favorite candies. But these were a melted mess.  Changes texture and taste. Price was good but not worth it.  Also could not return.  Booooooooooo....   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Text_Cleaned   \n",
       "Id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "90030                                                                                                                                                                                                           [come, packages, folded, sealed, shut, quality, uneven, perhaps, 20, pears, truly, soft, moist, rest, varying, shades, dried, buying, <, br, ><, br, >, i, received, dried, pears, present, years, ago, pears, uniformly, big, moist, juicy, wish, remembered, brand, cause, good, dried, pears, wonderful]  \\\n",
       "84578   [hot, sauce, one, kind, always, hesitant, try, spicy, foods, taste, hot, sauce, tried, rocky, transformed, hot, sauces, seem, chemically, rocky, homey, flavor, incredibly, diverse, much, using, eggs, tacos, marinade, chicken, grilled, cheese, family, six, bottles, rockys, last, two, months, honestly, say, first, family, certainly, last, rocky, become, staple, table, every, meal, thank, creating, amazing, hot, sauce, tried, order, <, br, ><, br, >, lime, notes, zingy, fresh, lime, awful, conc...   \n",
       "390282  [nothing, like, coca, cola, real, sugar, live, passover, time, get, kosher, coke, local, supermarket, also, real, sugar, times, like, grab, mexi, coke, much, better, regular, coke, readily, available, local, supermarket, place, close, home, could, get, might, consider, paying, 21, amazon, since, go, wegman, get, whole, case, 24, bottles, 1, less, costs, 6, bottles, pass, 5, star, product, 1, star, price, split, difference, 3, stars, review, wegman, near, consider, taking, advantage, offer, w...   \n",
       "125128                                                                  [remember, first, time, tried, lindt, 70, bar, visited, france, child, parents, bought, several, bring, back, states, cherished, eaten, slowly, enjoyed, special, treat, ever, since, <, br, ><, br, >, sadly, era, ended, new, recipe, shadow, former, lindt, chocolate, tastes, like, pretty, much, standard, dark, chocolate, bar, none, crisp, vibrant, complexity, used, <, br, ><, br, >, i, guess, good, things, eventually, come, end, bad]   \n",
       "90827                                                                                                                                                                                                                                                                                                                                                                     [reese, eggs, taste, great, one, favorite, candies, melted, mess, changes, texture, taste, price, good, worth, also, could, return, booooooooooo]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Lemmatized_text  \n",
       "Id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "90030                                                                                                                                                                                                                  [come, package, folded, sealed, shut, quality, uneven, perhaps, 20, pear, truly, soft, moist, rest, varying, shade, dried, buying, <, br, ><, br, >, i, received, dried, pear, present, year, ago, pear, uniformly, big, moist, juicy, wish, remembered, brand, cause, good, dried, pear, wonderful]  \n",
       "84578   [hot, sauce, one, kind, always, hesitant, try, spicy, food, taste, hot, sauce, tried, rocky, transformed, hot, sauce, seem, chemically, rocky, homey, flavor, incredibly, diverse, much, using, egg, taco, marinade, chicken, grilled, cheese, family, six, bottle, rockys, last, two, month, honestly, say, first, family, certainly, last, rocky, become, staple, table, every, meal, thank, creating, amazing, hot, sauce, tried, order, <, br, ><, br, >, lime, note, zingy, fresh, lime, awful, concentrate...  \n",
       "390282  [nothing, like, coca, cola, real, sugar, live, passover, time, get, kosher, coke, local, supermarket, also, real, sugar, time, like, grab, mexi, coke, much, better, regular, coke, readily, available, local, supermarket, place, close, home, could, get, might, consider, paying, 21, amazon, since, go, wegman, get, whole, case, 24, bottle, 1, le, cost, 6, bottle, pas, 5, star, product, 1, star, price, split, difference, 3, star, review, wegman, near, consider, taking, advantage, offer, wegman, c...  \n",
       "125128                                                                      [remember, first, time, tried, lindt, 70, bar, visited, france, child, parent, bought, several, bring, back, state, cherished, eaten, slowly, enjoyed, special, treat, ever, since, <, br, ><, br, >, sadly, era, ended, new, recipe, shadow, former, lindt, chocolate, taste, like, pretty, much, standard, dark, chocolate, bar, none, crisp, vibrant, complexity, used, <, br, ><, br, >, i, guess, good, thing, eventually, come, end, bad]  \n",
       "90827                                                                                                                                                                                                                                                                                                                                                                         [reese, egg, taste, great, one, favorite, candy, melted, mess, change, texture, taste, price, good, worth, also, could, return, booooooooooo]  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('max_colwidth', 500)\n",
    "df[['Score', 'Text', 'Text_Cleaned', 'Lemmatized_text']].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed297b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cleaned text has: 110421 words\n"
     ]
    }
   ],
   "source": [
    "bow_converter:sklearn.feature_extraction.text.CountVectorizer = CountVectorizer(tokenizer=lambda doc: doc, ngram_range=(1,1), lowercase=False)\n",
    "x:scipy.sparse.csr.csr_matrix = bow_converter.fit_transform(df['Text_Cleaned'])\n",
    "\n",
    "words:List[str] = bow_converter.get_feature_names_out()\n",
    "print(f'The cleaned text has: {len(words)} words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ae433fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow:sklearn.feature_extraction.text.CountVectorizer = CountVectorizer(tokenizer=lambda doc: doc, ngram_range=(1,1), lowercase=False)\n",
    "X1:scipy.sparse.csr.csr_matrix = bow.fit_transform(df['Text_Cleaned'])\n",
    "y1:np.ndarray = df['Score']\n",
    "    \n",
    "X1 = X1[:1000]\n",
    "y1 = y1[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13e83648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ElasticNet(random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ElasticNet</label><div class=\"sk-toggleable__content\"><pre>ElasticNet(random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "ElasticNet(random_state=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_reg:sklearn.linear_model._base.LinearRegression = LinearRegression()\n",
    "ridge:sklearn.linear_model._ridge.Ridge = Ridge(alpha=1.0)\n",
    "lasso:sklearn.linear_model._coordinate_descent.Lasso = Lasso(alpha=1.0)\n",
    "elastic_net:sklearn.linear_model._coordinate_descent.ElasticNet = ElasticNet(random_state=0)\n",
    "\n",
    "linear_reg.fit(X1, y1)\n",
    "ridge.fit(X1, y1)\n",
    "lasso.fit(X1, y1)\n",
    "elastic_net.fit(X1, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c51aaa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_reg_params() -> Dict[str,List[bool]]:\n",
    "    \"\"\"\n",
    "    Creates a dictionary with the names of the parameters as keys, and as values,\n",
    "    lists with the possible values; for Linear Regression\n",
    "    return: the dictionary \n",
    "    \"\"\"\n",
    "    copy_X:List[bool] = [True,False]\n",
    "    fit_intercept:List[bool] = [True,False]\n",
    "    return dict(copy_X=copy_X, fit_intercept=fit_intercept)\n",
    "\n",
    "def lasso_ridge_elastic_params() -> Dict[str,List[Union[float,bool]]]:\n",
    "    \"\"\"\n",
    "    Creates a dictionary with the names of the parameters as keys, and as values,\n",
    "    lists with the possible values; for Lasso, Ridge and Elastic Net\n",
    "    return: the dictionary\n",
    "    \"\"\"\n",
    "    alpha:List[float] = [1.0,1.1,1.2,1.3,1.4,1.5]\n",
    "    fit_intercept:List[bool] = [True,False]\n",
    "    return dict(alpha=alpha, fit_intercept=fit_intercept)\n",
    "\n",
    "def randomized_search(model:sklearn.linear_model, param_distributions:Dict[str,List[Union[float,bool]]], data:np.ndarray, target:np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the optimal hyperparameters values for a chosen regression model, for a given data set\n",
    "    param model: the regression model\n",
    "    param param_distributions: dictionary with the names of the parameters as the value of the keys and a set of values\n",
    "    param date: the date of the data set\n",
    "    param target: the target of the data set\n",
    "    \"\"\"\n",
    "    randomized_src:sklearn.model_selection.RandomizedSearchCV = RandomizedSearchCV(estimator = model, param_distributions=param_distributions, n_iter = 5, scoring='neg_mean_squared_error', cv=5, return_train_score=True)\n",
    "    randomized_src.fit(data, target)\n",
    "    return randomized_src\n",
    "\n",
    "def randomize_calculate(model:sklearn.linear_model, params:Dict[str,List[Union[float,bool]]], data:np.ndarray, target:np.ndarray, score_type:str, error_type:str) -> float:\n",
    "    \"\"\"\n",
    "    Reports the mean results for one of the fold links (train / test) with RandomizedSearchCV\n",
    "    param model: the regression model\n",
    "    param param_grid: dictionary with the name of the parameters as the value of the keys and a set of values\n",
    "    param date: the data of the data set\n",
    "    param target: the target of the data set\n",
    "    param score_type: the type of score\n",
    "    param error_type: the type of error\n",
    "    \"\"\"\n",
    "    rand_src:sklearn.model_selection.RandomizedSearchCV = randomized_search(model, params, data, target)\n",
    "    result:List[float] = cross_validate(rand_src, data, target, cv=5, return_train_score=True, scoring=error_type)\n",
    "    return result[score_type].mean()\n",
    "\n",
    "def get_errors_score(score_type: str, error_type: str, data:np.ndarray, target:np.ndarray) -> List[float]:\n",
    "    \"\"\"\n",
    "    Builds a list from the mean results for the training folds, as well as for the test folds,\n",
    "    for a given dataset\n",
    "    param score_type: the type of score\n",
    "    param error_type: type of error\n",
    "    param date: the date of the dataset\n",
    "    param target: the target of the dataset\n",
    "    return: list of averages\n",
    "    \"\"\"\n",
    "    column:List[float] = []\n",
    "    column.append(randomize_calculate(linear_reg, lin_reg_params(), data, target, score_type, error_type))\n",
    "    column.append(randomize_calculate(ridge, lasso_ridge_elastic_params(), data, target, score_type, error_type))  \n",
    "    column.append(randomize_calculate(lasso, lasso_ridge_elastic_params(), data, target, score_type, error_type))   \n",
    "    column.append(randomize_calculate(elastic_net, lasso_ridge_elastic_params(), data, target, score_type, error_type))   \n",
    "    return column\n",
    "\n",
    "def get_data_frame(data:np.ndarray, target:np.ndarray) -> Dict[str,List[Union[str,float]]]:\n",
    "    \"\"\"\n",
    "    Builds a dictionary from the columns of the result averages for the training folds,\n",
    "    as well as for the test ones, for a given dataset\n",
    "    param date: the data of the dataset\n",
    "    param target: the target of the dataset\n",
    "    return: the dictionary\n",
    "    \"\"\"\n",
    "    test_neg_mean_absolute_error:List[float] = get_errors_score('test_score', 'neg_mean_absolute_error', data, target)    \n",
    "    test_neg_mean_squared_error:List[float] = get_errors_score('test_score', 'neg_mean_squared_error', data, target)\n",
    "    test_r2_error:List[float] = get_errors_score('test_score', 'r2', data, target)\n",
    "    train_neg_mean_absolute_error:List[float] = get_errors_score('train_score', 'neg_mean_absolute_error', data, target)\n",
    "    train_neg_mean_squared_error:List[float] = get_errors_score('train_score', 'neg_mean_squared_error', data, target)\n",
    "    train_r2_error:List[float] = get_errors_score('train_score', 'r2', data, target)\n",
    "\n",
    "    data_frame:Dict[str,List[Union[str,float]]] = {\n",
    "            'Model_name': ['Linear Regression', 'Ridge', 'Lasso', 'Elastic Net'],\n",
    "            'Search_strategy': ['RandomizedSearchCV', 'RandomizedSearchCV', 'RandomizedSearchCV','RandomizedSearchCV'],\n",
    "            'test_neg_mean_absolute_error': test_neg_mean_absolute_error,\n",
    "            'test_neg_mean_squared_error': test_neg_mean_squared_error,\n",
    "            'test_r2_error': test_r2_error,\n",
    "            'train_neg_mean_absolute_error': train_neg_mean_absolute_error,\n",
    "            'train_neg_mean_squared_error': train_neg_mean_squared_error,\n",
    "            'train_r2_error': train_r2_error,\n",
    "        }\n",
    "    return data_frame\n",
    "\n",
    "def get_positive_data_frame(data_frame: Dict[str,List[Union[str,float]]]) -> Dict[str,List[Union[str,float]]]:\n",
    "    \"\"\"\n",
    "    Builds a dictionary with positive values from the columns made up from the result averages\n",
    "    for training folds, as well as for the test ones, for a given dataset\n",
    "    param date: the data of the dataset\n",
    "    param target: the target of the dataset\n",
    "    return: the dictionary\n",
    "    \"\"\"\n",
    "    pos_data_frame:Dict[str,List[Union[str,float]]] = {}\n",
    "    lst:List[Union[str,float]] = []\n",
    "    for key in data_frame:\n",
    "        for value in data_frame[key]:\n",
    "            if isinstance(value, float):\n",
    "                lst.append(abs(value))\n",
    "            else:\n",
    "                lst.append(value)\n",
    "        pos_data_frame[key.replace(\"_neg\",\"\")] = lst\n",
    "        lst = []\n",
    "    return pos_data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed3d511",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame:Dict[str,List[Union[str,float]]] = get_data_frame(X1, y1)\n",
    "new_data_frame:pandas.core.frame.DataFrame = pd.DataFrame(data_frame)\n",
    "display(new_data_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf38d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_data_frame:Dict[str,List[Union[str,float]]] = get_positive_data_frame(data_frame)\n",
    "pos_df:pandas.core.frame.DataFrame = pd.DataFrame(positive_data_frame)\n",
    "display(pos_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a970065b",
   "metadata": {},
   "outputs": [],
   "source": [
    "style:pandas.io.formats.style.Styler = pos_df.style.\\\n",
    "    highlight_max(color = 'green', axis = 0).\\\n",
    "    highlight_min(color = 'red', axis = 0)\n",
    "\n",
    "style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebf97bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2:object = pd.read_csv('./data/Corona_NLP.csv', index_col=0)\n",
    "df2= df2.dropna()\n",
    "print(f'The dataframe shape is: {df2.shape}')\n",
    "df2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d09c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.drop_duplicates(subset={'ScreenName'}, inplace=True)\n",
    "print(f'The dataframe shape after removing the duplicate rows is: {df2.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739eb623",
   "metadata": {},
   "source": [
    "Grafic care ne arata care tweet-uri date de clienti sunt majoritare. <br>\n",
    "Se observa predominanta celor cu scorul negativ (>800). <br>\n",
    "Putem imparti tweet-urile in categorii in functie de scorul acestora dupa cum urmeaza: <br>\n",
    "Sentiment: Extremely Positive, Positive -> tweet bun <br>\n",
    "Sentiment: Neutral -> tweet neutru <br>\n",
    "Sentiment: Extremely Negative, Negative -> tweet negativ <br>\n",
    "Se observa o oarecare egalitate, negative (aprox. 1250), pozitive (aprox. 1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b2e7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax2:object = df2['Sentiment'].value_counts().plot(kind='bar', figsize=(7,7))\n",
    "fig2:object = ax2.get_figure()\n",
    "ax2.set_title(\"Corona Tweets Dataset\")\n",
    "ax2.set_xlabel('Sentiment')\n",
    "ax2.set_ylabel('Value Counts');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a952bb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('A concise summary of the DataFrame is:')\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cde9cd2",
   "metadata": {},
   "source": [
    "Dorim sa aflam si alte informatii despre tweet-uri, cum ar fi: <br>\n",
    "Cel mai lung, cel mai scurt tweet <br>\n",
    "Aceste date ne ajuta sa avem o mai buna privire de ansamblu asupra setului de date cu care lucram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344004df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The longest tweet written is: {df2.OriginalTweet.map(lambda x: len(x)).max()}')\n",
    "print(f'The shortest tweet written is: {df2.OriginalTweet.map(lambda x: len(x)).min()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c793a3",
   "metadata": {},
   "source": [
    "Grafic care grupeaza tweet-urile in functie de data cand au fost scrise. <br>\n",
    "Se observa faptul ca cele mai multe tweet-uri au fost scrise in data de 13.03.2020. <br>\n",
    "Cel mai probabil un anumit eveniment s-a intamplat in acea zi, de acolo si diferenta fata de celelalte date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f8b37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax2:object = df2['TweetAt'].value_counts().plot(kind='bar', figsize=(7,7))\n",
    "fig2:object = ax2.get_figure()\n",
    "ax2.set_title(\"Corona Tweets Dataset\")\n",
    "ax2.set_xlabel('TweetDate')\n",
    "ax2.set_ylabel('Value Counts');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88afafa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Text_Cleaned'] = list(map(clean_text, df2.OriginalTweet))\n",
    "def lemmatized_words(text): \n",
    "    lemm:nltk.stem.wordnet.WordNetLemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    df2['Lemmatized_text'] = list(map(lambda word:list(map(lemm.lemmatize, word)),df2.Text_Cleaned))\n",
    "\n",
    "lemmatized_words(df2.Text_Cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac291f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4918e890",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_race (row:List[Union[str,int]]) -> int:\n",
    "    \"\"\"\n",
    "    Function that converts reviews from string to integer\n",
    "    param row: the row with the review written as string\n",
    "    return: the score as an integer\n",
    "    \"\"\"\n",
    "    if row['Sentiment'] == 'Extremely Negative':\n",
    "        return 1\n",
    "    if row['Sentiment'] == 'Negative':\n",
    "        return 2\n",
    "    if row['Sentiment'] == 'Neutral':\n",
    "        return 3\n",
    "    if row['Sentiment'] == 'Positive':\n",
    "        return 4\n",
    "    if row['Sentiment'] == 'Extremely Positive':\n",
    "        return 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce8318e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Score'] = df2.apply(lambda row: label_race(row),axis=1)\n",
    "df2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a9ab1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_converter2:sklearn.feature_extraction.text.CountVectorizer = CountVectorizer(tokenizer=lambda doc: doc, ngram_range=(1,1), lowercase=False)\n",
    "x2:scipy.sparse.csr.csr_matrix = bow_converter2.fit_transform(df2['Text_Cleaned'])\n",
    "\n",
    "words2:List[str] = bow_converter2.get_feature_names_out()\n",
    "print(f'The cleaned text has: {len(words)} words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30a7240",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow2:sklearn.feature_extraction.text.CountVectorizer = CountVectorizer(tokenizer=lambda doc: doc, ngram_range=(1,1), lowercase=False)\n",
    "X2:scipy.sparse.csr.csr_matrix = bow2.fit_transform(df2['Text_Cleaned'])\n",
    "y2:np.ndarray = df2['Score']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6506a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_reg.fit(X2, y2)\n",
    "ridge.fit(X2, y2)\n",
    "lasso.fit(X2, y2)\n",
    "elastic_net.fit(X2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d98514",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame2:Dict[str,List[Union[str,float]]] = get_data_frame(X2, y2)\n",
    "new_data_frame2:pandas.core.frame.DataFrame = pd.DataFrame(data_frame2)\n",
    "display(new_data_frame2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26af8d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_data_frame2:Dict[str,List[Union[str,float]]] = get_positive_data_frame(data_frame2)\n",
    "pos_df2:pandas.core.frame.DataFrame = pd.DataFrame(positive_data_frame2)\n",
    "display(pos_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998919c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "style2:pandas.io.formats.style.Styler = pos_df.style.\\\n",
    "    highlight_max(color = 'green', axis = 0).\\\n",
    "    highlight_min(color = 'red', axis = 0)\n",
    "\n",
    "style2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a05a07",
   "metadata": {},
   "source": [
    "## Preprocesarea textului\n",
    "A preprocesa textul înseamnă pur și simplu sa-l aduce într-o formă care este previzibilă și analizabilă pentru o anumita sarcina. O sarcină aici este o combinație de abordare și domeniu. De exemplu, extragerea cuvintelor cheie de top cu TF-IDF (abordare) din Tweets (domeniu) este un exemplu de activitate.\n",
    "\n",
    "### Lower case:\n",
    "Scrierea cu minusculă a tuturor datelor textului este una dintre cele mai simple și mai eficiente forme de preprocesare. Este aplicabila majorității problemelor de extragere, iar NLP poate ajuta în cazurile în care setul dvs. de date nu este foarte mare. Ajută semnificativ la coerența rezultatului așteptat.\n",
    "\n",
    "### Contractions:\n",
    "Contracțiile sunt cuvinte sau combinații de cuvinte care sunt scurtate prin scăderea literelor și înlocuirea lor cu un apostrof.\n",
    "\n",
    "În zilele noastre, acolo unde totul se schimbă online, comunicăm mai mult cu ceilalți prin mesaje text sau postări pe diferite rețele sociale precum Facebook, Instagram, Whatsapp, Twitter, LinkedIn etc. sub formă de texte. Având atât de mulți oameni cu care să vorbim, ne bazăm pe abrevieri și forme scurte de cuvinte pentru a trimite mesaje text.\n",
    "Am înlocuit contracțiile cu formele lor mai lungi, cum ar fi “isn’t”: “is not”, “can’t”: “cannot“. Pentru a face acest lucru, am importat lista de contracții de [aici](https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python)\n",
    "\n",
    "### Remove special characters:\n",
    "Adesea, textul nestructurat conține o mulțime de zgomot, mai ales dacă utilizați tehnici precum screen scraping.\n",
    "Am curățat datele de orice caracter special, cum ar fi ghilimelele, punctuația și pronumele posesiv.\n",
    "\n",
    "### Stopwords: \n",
    "Cuvintele stop sunt un set de cuvinte utilizate în mod obișnuit într-o limbă. Exemple de cuvinte stop în engleză sunt „a”, „the”, „is”, „are” și etc. .\n",
    "\n",
    "De exemplu, în contextul unui sistem de căutare, dacă interogarea dvs. de căutare este „ce este preprocesarea textului?”, Doriți ca sistemul de căutare să se concentreze pe suprafața documentelor care vorbesc despre preprocesarea textului peste documentele care vorbesc despre ce este. Acest lucru se poate face prin prevenirea analizei tuturor cuvintelor din lista dvs. de cuvinte stop. Cuvintele stop sunt aplicate în mod obișnuit în sistemele de căutare, aplicațiile de clasificare a textului, modelarea subiectelor, extragerea subiectului și altele.\n",
    "Am eliminat cuvintele de oprire, deoarece acestea adaugă zgomot fără a aduce nicio valoare informațională în modelare. Am descărcat o listă de cuvinte cheie în engleză din pachetul nltk și le-am șters din corpusul de text.\n",
    "\n",
    "### Tokenization:\n",
    "Pentru a procesa textul, trebuie să îl împărțim în bucăți mai mici. Aici am împărțit propoziții în cuvinte folosind WordPunctTokenizer din biblioteca nltk.\n",
    "\n",
    "### Lemmatization:\n",
    "Lematizarea la suprafață este foarte asemănătoare cu stemming-ul, unde scopul este de a elimina inflexiunile și de a mapa un cuvânt la forma sa rădăcină. Singura diferență este că lematizarea încearcă să o facă în mod corespunzător. Nu doar toacă lucrurile, ci transformă de fapt cuvintele în rădăcina reală. De exemplu, cuvântul „better” ar fi asociat cu „good”. Poate folosi un dicționar precum WordNet pentru mapări sau unele abordări speciale bazate pe reguli. \n",
    "Pentru a converti fiecare cuvânt în cuvântul său rădăcină, am folosit Lemmatizer din WordNet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985266f4",
   "metadata": {},
   "source": [
    "## Modele de regresie folosite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a889557f",
   "metadata": {},
   "source": [
    "1. [Linear Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "2. [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)\n",
    "3. [Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)\n",
    "4. [Elastic Net](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5fa5d2",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2c7b3e",
   "metadata": {},
   "source": [
    "În statistici, regresia liniară este o abordare liniară pentru modelarea relației dintre un răspuns scalar și una sau mai multe variabile explicative (cunoscute și ca variabile dependente și independente). Cazul unei variabile explicative se numește regresie liniară simplă; pentru mai mult de unul, procesul se numește regresie liniară multiplă. Acest termen este distinct de regresia liniară multivariată, unde sunt prezise variabile dependente corelate multiple, mai degrabă decât o singură variabilă scalară.\n",
    "\n",
    "În regresia liniară, relațiile sunt modelate folosind funcții predictive liniare ale căror parametri de model necunoscuți sunt estimate din date. Astfel de modele se numesc modele liniare. Cel mai frecvent, media condițională a răspunsului dat de valorile variabilelor explicative (sau predictori) este presupusă a fi o funcție afină a acestor valori; mai rar, se utilizează mediana condițională sau o altă cuantilă. La fel ca toate formele de analiză de regresie, regresia liniară se concentrează pe distribuția condițională a probabilității răspunsului dat de valorile predictorilor, mai degrabă decât pe distribuția comună a probabilității tuturor acestor variabile, care este domeniul analizei multivariate.\n",
    "\n",
    "Regresia liniară a fost primul tip de analiză de regresie care a fost studiată riguros și care a fost utilizată pe scară largă în aplicații practice. Acest lucru se datorează faptului că modelele care depind liniar de parametrii lor necunoscuți sunt mai ușor de adaptat decât modelele care sunt neliniar legate de parametrii lor și deoarece proprietățile statistice ale estimatorilor rezultați sunt mai ușor de determinat.\n",
    "\n",
    "Regresia liniară are multe utilizări practice. Majoritatea aplicațiilor se încadrează în una dintre următoarele două categorii mari:\n",
    "\n",
    "Dacă obiectivul este predicția, prognozarea sau reducerea erorilor, regresia liniară poate fi utilizată pentru a se potrivi un model predictiv la un set de date observat de valori ale răspunsului și variabilele explicative. După dezvoltarea unui astfel de model, dacă valorile suplimentare ale variabilelor explicative sunt colectate fără o valoare de răspuns însoțitoare, modelul adaptat poate fi utilizat pentru a face o predicție a răspunsului.\n",
    "\n",
    "Dacă scopul este de a explica variația variabilei de răspuns care poate fi atribuită variației variabilelor explicative, analiza regresiei liniare poate fi aplicată pentru a cuantifica puterea relației dintre răspuns și variabilele explicative și, în special, pentru a determina dacă unele variabilele explicative pot să nu aibă deloc o relație liniară cu răspunsul sau să identifice care subseturi de variabile explicative pot conține informații redundante despre răspuns.\n",
    "\n",
    "Modelele de regresie liniară sunt adesea montate utilizând abordarea cu cel mai mic pătrat, dar pot fi montate și în alte moduri, cum ar fi prin minimizarea „lipsei de potrivire” în alte norme (cum ar fi regresia cu deviații absolute minime) sau prin minimizarea unei penalizări versiunea funcției costului celor mai mici pătrate ca în regresia crestei (penalizare L2-normă) și lazo (penalizare L1-normă). În schimb, abordarea celor mai mici pătrate poate fi utilizată pentru a se potrivi modelelor care nu sunt modele liniare. Astfel, deși termenii „cele mai mici pătrate” și „modelul liniar” sunt strâns legate, nu sunt sinonime.\n",
    "\n",
    "**Bibliografie:** <br>\n",
    "    https://en.wikipedia.org/wiki/Linear_regression <br>\n",
    "    https://machinelearningmastery.com/linear-regression-for-machine-learning/\n",
    "    \n",
    " ![LinearRegression](./images/LinearRegression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1ee59b",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5f63ba",
   "metadata": {},
   "source": [
    "Regresia Ridge este o metodă de estimare a coeficienților modelelor cu regresie multiplă în scenarii în care variabilele independente sunt puternic corelate. Are utilizări în domenii, inclusiv econometrie, chimie și inginerie.\n",
    "\n",
    "Teoria a fost introdusă pentru prima dată de Hoerl și Kennard în 1970 în lucrările lor tehnometrice „Regresii RIDGE: estimare părtinitoare a problemelor nonortogonale” și „Regresii RIDGE: aplicații în probleme nonortagonale”. Acesta a fost rezultatul a zece ani de cercetări în domeniul analizei crestelor.\n",
    "\n",
    "Regresia Ridge a fost dezvoltată ca o posibilă soluție la imprecizia estimatorilor cu cel mai mic pătrat atunci când modelele de regresie liniară au unele variabile independente multicoliniare (foarte corelate) - prin crearea unui estimator de regresie a crestei (RR). Aceasta oferă o estimare mai precisă a parametrilor de creastă, deoarece varianța și estimatorul pătrat mediu sunt adesea mai mici decât cei mai mici estimatori derivați anterior.\n",
    "\n",
    "Pentru orice tip de modele de învățare automată cu regresie, ecuația obișnuită de regresie formează baza care este scrisă ca:\n",
    "\n",
    "Y = XB + e\n",
    "\n",
    "În cazul în care Y este variabila dependentă, X reprezintă variabilele independente, B este coeficienții de regresie care trebuie evaluați și e reprezintă erorile sunt reziduuri.\n",
    "\n",
    "Odată ce adăugăm funcția lambda la această ecuație, se ia în considerare varianța care nu este evaluată de modelul general. După ce datele sunt gata și identificate pentru a face parte din regularizarea L2, există pași pe care îi puteți întreprinde.\n",
    "\n",
    "În regresia de creastă, primul pas este standardizarea variabilelor (atât dependente, cât și independente) prin scăderea mijloacelor lor și împărțirea la abaterile standard. Acest lucru provoacă o provocare în notație, deoarece trebuie cumva să indicăm dacă variabilele dintr-o anumită formulă sunt standardizate sau nu. În ceea ce privește standardizarea, toate calculele de regresie a crestei se bazează pe variabile standardizate. Când sunt afișați coeficienții de regresie finali, aceștia sunt reglați înapoi la scara lor originală. Cu toate acestea, urmele de creastă sunt pe o scară standardizată.\n",
    "\n",
    "**Bibliografie:** <br>\n",
    "    https://en.wikipedia.org/wiki/Ridge_regression <br>\n",
    "    https://towardsdatascience.com/ridge-regression-for-better-usage-2f19b3a202db\n",
    "    \n",
    "![Ridge](./images/Ridge.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7d2998",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526c2bfc",
   "metadata": {},
   "source": [
    "În statistici și învățarea automată, Lasso (cel mai mic operator de contracție și selecție absolut; de asemenea Lasso sau LASSO) este o metodă de analiză de regresie care efectuează atât selecția variabilă, cât și regularizarea, pentru a spori precizia de predicție și interpretabilitatea modelului statistic rezultat. A fost inițial introdus în geofizică, și mai târziu de Robert Tibshirani, care a inventat termenul.\n",
    "\n",
    "Lasso a fost inițial formulat pentru modele de regresie liniară. Acest caz simplu relevă o cantitate substanțială despre estimator. Acestea includ relația sa cu regresia crestei și cea mai bună selecție a subsetului și conexiunile dintre estimările coeficientului lazo și așa-numitul prag moale. De asemenea, arată că (cum ar fi regresia liniară standard) estimările coeficientului nu trebuie să fie unice dacă covariabilele sunt coliniare.\n",
    "\n",
    "Deși inițial definită pentru regresie liniară, regularizarea lasso este ușor extinsă la alte modele statistice, inclusiv modele liniare generalizate, ecuații de estimare generalizate, modele de pericole proporționale și estimatori M. Capacitatea lui Lasso de a efectua selecția subsetului se bazează pe forma constrângerii și are o varietate de interpretări, inclusiv în ceea ce privește geometria, statisticile bayesiene și analiza convexă.\n",
    "\n",
    "LASSO este strâns legat de denoising de urmărire de bază.\n",
    "\n",
    "Regresia lasso realizează regularizarea L1, care adaugă o penalizare egală cu valoarea absolută a magnitudinii coeficienților. Acest tip de regularizare poate duce la modele rare, cu puțini coeficienți; Unii coeficienți pot deveni zero și eliminați din model. Penalitățile mai mari au ca rezultat valori ale coeficientului mai apropiate de zero, ceea ce este ideal pentru producerea de modele mai simple. Pe de altă parte, regularizarea L2 (de exemplu regresia Ridge) nu are ca rezultat eliminarea coeficienților sau a modelelor rare. Acest lucru face ca Lasso să fie mult mai ușor de interpretat decât Ridge. Soluțiile lazo sunt probleme de programare pătratică, care sunt cel mai bine rezolvate cu software-ul.\n",
    "\n",
    "O problemă cu regresia liniară este că coeficienții estimate ai modelului pot deveni mari, făcând modelul sensibil la intrări și, eventual, instabil. Acest lucru este valabil mai ales pentru problemele cu puține observații (eșantioane) sau mai multe eșantioane (n) decât predictori de intrare (p) sau variabile (așa-numitele probleme p >> n).\n",
    "\n",
    "O abordare pentru abordarea stabilității modelelor de regresie este schimbarea funcției de pierdere pentru a include costuri suplimentare pentru un model care are coeficienți mari. Modelele de regresie liniară care utilizează aceste funcții de pierdere modificate în timpul antrenamentului sunt denumite colectiv regresie liniară penalizată.\n",
    "\n",
    "O penalizare populară este de a penaliza un model bazat pe suma valorilor coeficientului absolut. Aceasta se numește penalizare L1. O penalizare L1 minimizează dimensiunea tuturor coeficienților și permite ca unii coeficienți să fie reduși la valoarea zero, ceea ce elimină predictorul din model.\n",
    "\n",
    "l1_penalty = suma j = 0 la p abs (beta_j)\n",
    "O penalizare L1 minimizează dimensiunea tuturor coeficienților și permite oricărui coeficient să meargă la valoarea zero, eliminând efectiv caracteristicile de intrare din model.\n",
    "\n",
    "Înainte de Lasso, cea mai utilizată metodă de alegere a covariabilelor a fost selecția în trepte. Această abordare îmbunătățește precizia predicției doar în anumite cazuri, cum ar fi atunci când doar câteva covariabile au o relație puternică cu rezultatul. Cu toate acestea, în alte cazuri, poate crește eroarea de predicție.\n",
    "\n",
    "**Bibliografie:** <br>\n",
    "https://en.wikipedia.org/wiki/Lasso_(statistics) <br>\n",
    "https://www.statisticshowto.com/lasso-regression/\n",
    "\n",
    "![Lasso](./images/Lasso.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986fc633",
   "metadata": {},
   "source": [
    "### Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885dd009",
   "metadata": {},
   "source": [
    "Elastic Net este un tip popular de regresie liniară regularizată care combină două penalizări populare, în special funcția de penalizare L1 și L2.\n",
    "\n",
    "Regresia liniară se referă la un model care presupune o relație liniară între variabilele de intrare și variabila țintă.\n",
    "\n",
    "Cu o singură variabilă de intrare, această relație este o linie și, cu dimensiuni mai mari, această relație poate fi gândită ca un hiperplan care conectează variabilele de intrare la variabila țintă. Coeficienții modelului se găsesc printr-un proces de optimizare care urmărește să minimizeze suma erorii pătrate dintre predicții (yhat) și valorile țintă așteptate (y).\n",
    "\n",
    "pierdere = suma i = 0 la n (y_i - yhat_i) ^ 2 <br>\n",
    "O problemă cu regresia liniară este că coeficienții estimate ai modelului pot deveni mari, făcând modelul sensibil la intrări și, eventual, instabil. Acest lucru este valabil mai ales pentru problemele cu puține observații (eșantioane) sau mai multe eșantioane (n) decât predictori de intrare (p) sau variabile (așa-numitele probleme p >> n).\n",
    "\n",
    "O abordare pentru abordarea stabilității modelelor de regresie este schimbarea funcției de pierdere pentru a include costuri suplimentare pentru un model care are coeficienți mari. Modelele de regresie liniară care utilizează aceste funcții de pierdere modificate în timpul antrenamentului sunt denumite colectiv regresie liniară penalizată.\n",
    "\n",
    "O penalizare populară este de a penaliza un model bazat pe suma valorilor coeficientului pătrat. Aceasta se numește penalizare L2. O penalizare L2 minimizează dimensiunea tuturor coeficienților, deși împiedică eliminarea oricăror coeficienți din model.\n",
    "\n",
    "l2_penalty = sum j = 0 to p beta_j ^ 2 <br>\n",
    "O altă penalizare populară este de a penaliza un model bazat pe suma valorilor coeficientului absolut. Aceasta se numește penalizare L1. O penalizare L1 minimizează dimensiunea tuturor coeficienților și permite ca unii coeficienți să fie reduși la valoarea zero, ceea ce elimină predictorul din model.\n",
    "\n",
    "l1_penalty = suma j = 0 la p abs (beta_j) <br>\n",
    "Elastic Net este un model de regresie liniar penalizat care include atât penalizările L1, cât și L2 în timpul antrenamentului.\n",
    "\n",
    "Folosind terminologia din „Elementele învățării statistice”, este furnizat un hiperparametru „alfa” pentru a atribui cât de multă greutate este acordată fiecăreia dintre penalitățile L1 și L2. Alfa este o valoare cuprinsă între 0 și 1 și este utilizată pentru a pondera contribuția penalizării L1 și cu un minus valoarea alfa este utilizată pentru a cântări penalitatea L2.\n",
    "\n",
    "elastic_net_penalty = (alfa * l1_penalty) + ((1 - alfa) * l2_penalty) <br>\n",
    "De exemplu, un alfa de 0,5 ar oferi o contribuție de 50% din fiecare penalizare la funcția de pierdere. O valoare alfa de 0 dă toată greutatea pedepsei L2 și o valoare 1 dă toată greutatea pedepsei L1.\n",
    "\n",
    "Parametrul alfa determină mixul penalizărilor și este adesea preselectat din motive calitative.\n",
    "\n",
    "Avantajul este că rețeaua elastică permite un echilibru al ambelor penalizări, ceea ce poate duce la o performanță mai bună decât un model cu una sau cealaltă penalizare pe anumite probleme.\n",
    "\n",
    "Este furnizat un alt hiperparametru numit „lambda” care controlează ponderarea sumei ambelor penalități la funcția de pierdere. O valoare implicită de 1,0 este utilizată pentru a utiliza penalizarea complet ponderată; o valoare 0 exclude penalizarea. Valori foarte mici ale lambadei, cum ar fi 1e-3 sau mai mici, sunt frecvente.\n",
    "\n",
    "elastic_net_loss = loss + (lambda * elastic_net_penalty)\n",
    "\n",
    "**Bibliografie:** <br>\n",
    "https://en.wikipedia.org/wiki/Elastic_net_regularization <br>\n",
    "https://machinelearningmastery.com/elastic-net-regression-in-python/\n",
    "\n",
    "![Elastic Net](./images/ElasticNet.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cbba0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
