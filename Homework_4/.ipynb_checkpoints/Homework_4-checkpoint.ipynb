{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35d89cb7",
   "metadata": {},
   "source": [
    "1. (20 points) Apply a missing value imputation method, where applicable; document the method used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b641ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sepal_length  sepal_width  petal_length  petal_width           class\n",
      "0             5.1          3.5           NaN          NaN     Iris-setosa\n",
      "1             4.9          NaN           NaN          0.2     Iris-setosa\n",
      "2             4.7          NaN           NaN          NaN     Iris-setosa\n",
      "3             4.6          NaN           NaN          0.2     Iris-setosa\n",
      "4             5.0          3.6           1.4          0.2     Iris-setosa\n",
      "..            ...          ...           ...          ...             ...\n",
      "145           6.7          3.0           5.2          NaN  Iris-virginica\n",
      "146           6.3          2.5           5.0          NaN  Iris-virginica\n",
      "147           NaN          3.0           5.2          NaN  Iris-virginica\n",
      "148           NaN          3.4           5.4          2.3  Iris-virginica\n",
      "149           NaN          3.0           5.1          NaN  Iris-virginica\n",
      "\n",
      "[150 rows x 5 columns]\n",
      "     sepal_length  sepal_width  petal_length  petal_width           class\n",
      "0        5.100000          3.5           4.4         88.0     Iris-setosa\n",
      "1        4.900000          3.0           4.4          0.2     Iris-setosa\n",
      "2        4.700000          3.0           4.4         88.0     Iris-setosa\n",
      "3        4.600000          3.0           4.4          0.2     Iris-setosa\n",
      "4        5.000000          3.6           1.4          0.2     Iris-setosa\n",
      "..            ...          ...           ...          ...             ...\n",
      "145      6.700000          3.0           5.2         88.0  Iris-virginica\n",
      "146      6.300000          2.5           5.0         88.0  Iris-virginica\n",
      "147      5.836054          3.0           5.2         88.0  Iris-virginica\n",
      "148      5.836054          3.4           5.4          2.3  Iris-virginica\n",
      "149      5.836054          3.0           5.1         88.0  Iris-virginica\n",
      "\n",
      "[150 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from typing import List\n",
    "\n",
    "path: str = \"./data/iris.data\"\n",
    "names: List[str] = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n",
    "iris_df = pd.read_csv(path, names=names)\n",
    "\n",
    "imp0 = SimpleImputer(missing_values=np.NaN, strategy='mean')\n",
    "imp0.fit(iris_df[['sepal_length']])\n",
    "imp1 = SimpleImputer(missing_values=np.NaN, strategy='most_frequent')\n",
    "imp1.fit(iris_df[['sepal_width']])\n",
    "imp2 = SimpleImputer(missing_values=np.NaN, strategy='median')\n",
    "imp2.fit(iris_df[['petal_length']])\n",
    "imp3 = SimpleImputer(missing_values=np.NaN, strategy='constant', fill_value=88)\n",
    "imp3.fit(iris_df[['petal_width']])\n",
    "\n",
    "print(iris_df)\n",
    "iris_df['sepal_length'] = imp0.transform(iris_df[['sepal_length']])\n",
    "iris_df['sepal_width'] = imp1.transform(iris_df[['sepal_width']])\n",
    "iris_df['petal_length'] = imp2.transform(iris_df[['petal_length']])\n",
    "iris_df['petal_width'] = imp3.transform(iris_df[['petal_width']])\n",
    "print(iris_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c09f4998",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    from_node_id  to_node_id\n",
      "0            NaN           1\n",
      "1            NaN           2\n",
      "2            0.0          29\n",
      "3            NaN           0\n",
      "4            1.0          23\n",
      "5            NaN          32\n",
      "6            2.0           0\n",
      "7            NaN          26\n",
      "8            2.0          34\n",
      "9            NaN           0\n",
      "10           NaN         358\n",
      "11           NaN           1\n",
      "12          23.0          13\n",
      "    from_node_id  to_node_id\n",
      "0            5.6           1\n",
      "1            5.6           2\n",
      "2            0.0          29\n",
      "3            5.6           0\n",
      "4            1.0          23\n",
      "5            5.6          32\n",
      "6            2.0           0\n",
      "7            5.6          26\n",
      "8            2.0          34\n",
      "9            5.6           0\n",
      "10           5.6         358\n",
      "11           5.6           1\n",
      "12          23.0          13\n"
     ]
    }
   ],
   "source": [
    "path2:str = \"./data/roadNet-TX.txt\"\n",
    "names2:List[str] = ['from_node_id', 'to_node_id']\n",
    "nodes = pd.read_csv(path2, names=names2)\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit(nodes[['from_node_id']])\n",
    "\n",
    "print(nodes)\n",
    "nodes['from_node_id'] = imp.transform(nodes[['from_node_id']])\n",
    "print(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e484f7",
   "metadata": {},
   "source": [
    "2. (number of models * number of datasets * 1 point = 20 points) For each dataset apply 5 classification models from scikit learn. For each report: accuracy, precision, recall, F1 score - see [sklearn.metrics](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics), [Precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall) - using 5 fold cross validation. Report the mean results for both training and test folds. The runs will be done with fixed hyperparameter values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98496d04",
   "metadata": {},
   "source": [
    "3. (number of models * number of datasets * 1 point = 20 points) Report the performance of each model, using 5-fold cross validation. For each of the 5 runs, look for optimal hyperparameters using 4-fold cross validation. Model performance will be reported as the average of the 5 runs. \n",
    "    *Note:* for each of the 5 runs, the optimal hyperparameters may differ, due to the data used for training/validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9649ab7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  [0.96666667 1.         0.93333333 0.96666667 1.        ]\n",
      "precision_micro:  [0.96666667 1.         0.93333333 0.96666667 1.        ]\n",
      "recall_micro:  [0.96666667 1.         0.93333333 0.96666667 1.        ]\n",
      "f1_micro:  [0.96666667 1.         0.93333333 0.96666667 1.        ]\n",
      "\n",
      "Mean for train scores:  0.97\n",
      "Mean for test scores:  0.9733333333333334\n",
      "\n",
      "Accuracy performance: 0.9733333333333334\n",
      "Precision_micro performance: 0.9733333333333334\n",
      "Recall_micro performance: 0.9733333333333334\n",
      "F1_micro performance: 0.9733333333333334\n",
      "\n",
      "Max score obtained for: 3 with value: 0.9667496443812233\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.datasets import load_iris\n",
    "from typing import List\n",
    "\n",
    "def print_scores(score_to_print:dict) -> None:\n",
    "    \"\"\"\n",
    "    Displays the optimal performance for each model out of the following 4: accuracy, precision, recall, f1 score\n",
    "    param: score_to_print: retains the chosen classification models and the results of the 5 folds\n",
    "    \"\"\"\n",
    "    print('accuracy: ', score_to_print['test_accuracy'])\n",
    "    print('precision_micro: ', score_to_print['test_precision_micro'])\n",
    "    print('recall_micro: ', score_to_print['test_recall_micro'])\n",
    "    print('f1_micro: ', score_to_print['test_f1_micro'])\n",
    "    \n",
    "def print_performance(score_to_print:dict) -> None:\n",
    "    \"\"\"\n",
    "    Displays the optimal performance for each model out of the following 4: accuracy, precision, recall, f1 score\n",
    "    param: score_to_print: retains the chosen classification models and the results of the 5 folds\n",
    "    \"\"\"\n",
    "    print('Accuracy performance: ' + str(score_to_print['test_accuracy'].mean()))\n",
    "    print('Precision_micro performance: ' + str(score_to_print['test_precision_micro'].mean()))\n",
    "    print('Recall_micro performance: ' + str(score_to_print['test_recall_micro'].mean()))\n",
    "    print('F1_micro performance: ' + str(score_to_print['test_f1_micro'].mean()))\n",
    "    \n",
    "def mean_score_for_model(model_number:int, parameter:int, data_set_X:np.ndarray, data_set_y:np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Creates and trains a given model as a parameter\n",
    "    model parameter: number of the chosen classification model\n",
    "    param parameter: parameter used for the chosen classification model\n",
    "    return: average of scores for 4 fold CV \n",
    "    \"\"\"\n",
    "    if model_number == 1:\n",
    "        model = KNeighborsClassifier(n_neighbors=parameter)\n",
    "    elif model_number == 2:\n",
    "        model = DecisionTreeClassifier(random_state=parameter)\n",
    "    else:\n",
    "        model = RandomForestClassifier(max_depth=parameter, random_state=0)\n",
    "    scores = cross_val_score(model, data_set_X, data_set_y, cv=4, scoring='accuracy')\n",
    "    return scores.mean()\n",
    "\n",
    "def get_best_hyperparam(model_number:int, parameter:int, data_set_X:np.ndarray, data_set_y:np.ndarray) -> None:\n",
    "    \"\"\"\n",
    "    Find the optimal parameter \n",
    "    param model: number of the chosen classification model\n",
    "    param parameter: parameter used for the chosen classification model\n",
    "    \"\"\"\n",
    "    range_parameters:range = range(1, 15)\n",
    "    scores_parameters:List(float) = [mean_score_for_model(model,parameter,data_set_X,data_set_y) for parameter in range_parameters]\n",
    "    print('Max score obtained for: {0} with value: {1}'.format(1+np.argmax(scores_parameters), np.max(scores_parameters)))\n",
    "\n",
    "def print_everything(X:np.ndarray, y:np.ndarray, model) -> None:\n",
    "    \"\"\"\n",
    "    Perform 5 fold cross validation and print for each fold the accuracy, precision, recall, f1 score,\n",
    "    average scores, average scores for both training and test folds and last but not least\n",
    "    performance.\n",
    "    :param1: X - the date is retained\n",
    "    :param2: y - the target is retained\n",
    "    :param3: model - classification model used\n",
    "    \"\"\"\n",
    "    scores:List(str) = ['accuracy', 'precision_micro', 'recall_micro', 'f1_micro']        \n",
    "    score_to_print:dict = cross_validate(model, X, y, cv=5, scoring = scores)\n",
    "    print_scores(score_to_print)\n",
    "    print('')\n",
    "    results:dict = cross_validate(model, X, y, cv=5, return_train_score=True)\n",
    "    print('Mean for train scores: ', results['train_score'].mean())\n",
    "    print('Mean for test scores: ', results['test_score'].mean())\n",
    "    print('')\n",
    "    print_performance(score_to_print)\n",
    "    print('')\n",
    "    \n",
    "iris = load_iris()\n",
    "X:np.ndarray = iris.data\n",
    "y:np.ndarray = iris.target\n",
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "print_everything(X, y, model)\n",
    "get_best_hyperparam(1,5,X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9c3e14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  [0.96666667 0.96666667 0.9        0.96666667 1.        ]\n",
      "precision_micro:  [0.96666667 0.96666667 0.9        0.96666667 1.        ]\n",
      "recall_micro:  [0.96666667 0.96666667 0.9        0.96666667 1.        ]\n",
      "f1_micro:  [0.96666667 0.96666667 0.9        0.96666667 1.        ]\n",
      "\n",
      "Mean for train scores:  1.0\n",
      "Mean for test scores:  0.9600000000000002\n",
      "\n",
      "Accuracy performance: 0.9600000000000002\n",
      "Precision_micro performance: 0.9600000000000002\n",
      "Recall_micro performance: 0.9600000000000002\n",
      "F1_micro performance: 0.9600000000000002\n",
      "\n",
      "Max score obtained for: 3 with value: 0.9667496443812233\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=0)\n",
    "print_everything(X, y, model)\n",
    "get_best_hyperparam(2,0,X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c814e9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  [0.93333333 0.96666667 0.93333333 0.93333333 1.        ]\n",
      "precision_micro:  [0.93333333 0.96666667 0.93333333 0.93333333 1.        ]\n",
      "recall_micro:  [0.93333333 0.96666667 0.93333333 0.93333333 1.        ]\n",
      "f1_micro:  [0.93333333 0.96666667 0.93333333 0.93333333 1.        ]\n",
      "\n",
      "Mean for train scores:  0.9616666666666667\n",
      "Mean for test scores:  0.9533333333333334\n",
      "\n",
      "Accuracy performance: 0.9533333333333334\n",
      "Precision_micro performance: 0.9533333333333334\n",
      "Recall_micro performance: 0.9533333333333334\n",
      "F1_micro performance: 0.9533333333333334\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf = GaussianNB()\n",
    "clf.fit(X, y)\n",
    "print_everything(X,y,clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d7e40ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  [0.96666667 0.96666667 0.93333333 0.9        1.        ]\n",
      "precision_micro:  [0.96666667 0.96666667 0.93333333 0.9        1.        ]\n",
      "recall_micro:  [0.96666667 0.96666667 0.93333333 0.9        1.        ]\n",
      "f1_micro:  [0.96666667 0.96666667 0.93333333 0.9        1.        ]\n",
      "\n",
      "Mean for train scores:  0.96\n",
      "Mean for test scores:  0.9533333333333334\n",
      "\n",
      "Accuracy performance: 0.9533333333333334\n",
      "Precision_micro performance: 0.9533333333333334\n",
      "Recall_micro performance: 0.9533333333333334\n",
      "F1_micro performance: 0.9533333333333334\n",
      "\n",
      "Max score obtained for: 3 with value: 0.9667496443812233\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "rfc.fit(X, y)\n",
    "print_everything(X,y,rfc)\n",
    "get_best_hyperparam(3,2,X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee47f7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  [0.96666667 0.96666667 0.96666667 0.93333333 1.        ]\n",
      "precision_micro:  [0.96666667 0.96666667 0.96666667 0.93333333 1.        ]\n",
      "recall_micro:  [0.96666667 0.96666667 0.96666667 0.93333333 1.        ]\n",
      "f1_micro:  [0.96666667 0.96666667 0.96666667 0.93333333 1.        ]\n",
      "\n",
      "Mean for train scores:  0.975\n",
      "Mean for test scores:  0.9666666666666666\n",
      "\n",
      "Accuracy performance: 0.9666666666666666\n",
      "Precision_micro performance: 0.9666666666666666\n",
      "Recall_micro performance: 0.9666666666666666\n",
      "F1_micro performance: 0.9666666666666666\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "svc.fit(X, y)\n",
    "print_everything(X,y,svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f285204f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  [0.62295082 0.6557377  0.58333333 0.73333333 0.61666667]\n",
      "precision_micro:  [0.62295082 0.6557377  0.58333333 0.73333333 0.61666667]\n",
      "recall_micro:  [0.62295082 0.6557377  0.58333333 0.73333333 0.61666667]\n",
      "f1_micro:  [0.62295082 0.6557377  0.58333333 0.73333333 0.61666667]\n",
      "\n",
      "Mean for train scores:  0.7640650183464216\n",
      "Mean for test scores:  0.6424043715846995\n",
      "\n",
      "Accuracy performance: 0.6424043715846995\n",
      "Precision_micro performance: 0.6424043715846995\n",
      "Recall_micro performance: 0.6424043715846995\n",
      "F1_micro performance: 0.6424043715846995\n",
      "\n",
      "Max score obtained for: 3 with value: 0.8276754385964913\n"
     ]
    }
   ],
   "source": [
    "dataset2 = './data/heart.csv'\n",
    "data2 = pd.read_csv(dataset2)\n",
    "\n",
    "X2 = data2.values[:, :-1]\n",
    "y2 = data2.values[:, -1]\n",
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "print_everything(X2, y2,model)\n",
    "get_best_hyperparam(1,5,X2,y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f593a941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  [0.73770492 0.86885246 0.8        0.73333333 0.66666667]\n",
      "precision_micro:  [0.73770492 0.86885246 0.8        0.73333333 0.66666667]\n",
      "recall_micro:  [0.73770492 0.86885246 0.8        0.73333333 0.66666667]\n",
      "f1_micro:  [0.73770492 0.86885246 0.8        0.73333333 0.66666667]\n",
      "\n",
      "Mean for train scores:  1.0\n",
      "Mean for test scores:  0.7613114754098361\n",
      "\n",
      "Accuracy performance: 0.7613114754098361\n",
      "Precision_micro performance: 0.7613114754098361\n",
      "Recall_micro performance: 0.7613114754098361\n",
      "F1_micro performance: 0.7613114754098361\n",
      "\n",
      "Max score obtained for: 3 with value: 0.8276754385964913\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(random_state=0)\n",
    "print_everything(X2, y2, model)\n",
    "get_best_hyperparam(2,0,X2,y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68e36050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  [0.81967213 0.86885246 0.78333333 0.85       0.71666667]\n",
      "precision_micro:  [0.81967213 0.86885246 0.78333333 0.85       0.71666667]\n",
      "recall_micro:  [0.81967213 0.86885246 0.78333333 0.85       0.71666667]\n",
      "f1_micro:  [0.81967213 0.86885246 0.78333333 0.85       0.71666667]\n",
      "\n",
      "Mean for train scores:  0.8418641336031\n",
      "Mean for test scores:  0.8077049180327869\n",
      "\n",
      "Accuracy performance: 0.8077049180327869\n",
      "Precision_micro performance: 0.8077049180327869\n",
      "Recall_micro performance: 0.8077049180327869\n",
      "F1_micro performance: 0.8077049180327869\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(X2, y2)\n",
    "print_everything(X2,y2,clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8ffc092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  [0.81967213 0.86885246 0.85       0.83333333 0.8       ]\n",
      "precision_micro:  [0.81967213 0.86885246 0.85       0.83333333 0.8       ]\n",
      "recall_micro:  [0.81967213 0.86885246 0.85       0.83333333 0.8       ]\n",
      "f1_micro:  [0.81967213 0.86885246 0.85       0.83333333 0.8       ]\n",
      "\n",
      "Mean for train scores:  0.8650732142244779\n",
      "Mean for test scores:  0.8343715846994536\n",
      "\n",
      "Accuracy performance: 0.8343715846994536\n",
      "Precision_micro performance: 0.8343715846994536\n",
      "Recall_micro performance: 0.8343715846994536\n",
      "F1_micro performance: 0.8343715846994536\n",
      "\n",
      "Max score obtained for: 3 with value: 0.8276754385964913\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "rfc.fit(X2, y2)\n",
    "print_everything(X2,y2,rfc)\n",
    "get_best_hyperparam(3,2,X2,y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3febb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  [0.81967213 0.8852459  0.85       0.83333333 0.71666667]\n",
      "precision_micro:  [0.81967213 0.8852459  0.85       0.83333333 0.71666667]\n",
      "recall_micro:  [0.81967213 0.8852459  0.85       0.83333333 0.71666667]\n",
      "f1_micro:  [0.81967213 0.8852459  0.85       0.83333333 0.71666667]\n",
      "\n",
      "Mean for train scores:  0.9138849833681972\n",
      "Mean for test scores:  0.8209836065573771\n",
      "\n",
      "Accuracy performance: 0.8209836065573771\n",
      "Precision_micro performance: 0.8209836065573771\n",
      "Recall_micro performance: 0.8209836065573771\n",
      "F1_micro performance: 0.8209836065573771\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svc = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "svc.fit(X2, y2)\n",
    "print_everything(X2,y2,svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4027fe40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  [0.30880929 0.25750242 0.32236205 0.26550388 0.14437984]\n",
      "precision_micro:  [0.30880929 0.25750242 0.32236205 0.26550388 0.14437984]\n",
      "recall_micro:  [0.30880929 0.25750242 0.32236205 0.26550388 0.14437984]\n",
      "f1_micro:  [0.30880929 0.25750242 0.32236205 0.26550388 0.14437984]\n",
      "\n",
      "Mean for train scores:  0.48527752427608417\n",
      "Mean for test scores:  0.2597114973322227\n",
      "\n",
      "Accuracy performance: 0.2597114973322227\n",
      "Precision_micro performance: 0.2597114973322227\n",
      "Recall_micro performance: 0.2597114973322227\n",
      "F1_micro performance: 0.2597114973322227\n",
      "\n",
      "Max score obtained for: 14 with value: 0.8857132563543675\n"
     ]
    }
   ],
   "source": [
    "dataset3 = './data/Nationalities.txt'\n",
    "data3 = pd.read_csv(dataset3, delimiter = ',')\n",
    "\n",
    "X3 = data3.values[:, 1:]\n",
    "y3 = data3.values[:, 1]\n",
    "y3= y3.astype('int')\n",
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "print_everything(X3, y3, model)\n",
    "get_best_hyperparam(1,5,X3,y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18cac405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  [1. 1. 1. 1. 1.]\n",
      "precision_micro:  [1. 1. 1. 1. 1.]\n",
      "recall_micro:  [1. 1. 1. 1. 1.]\n",
      "f1_micro:  [1. 1. 1. 1. 1.]\n",
      "\n",
      "Mean for train scores:  1.0\n",
      "Mean for test scores:  1.0\n",
      "\n",
      "Accuracy performance: 1.0\n",
      "Precision_micro performance: 1.0\n",
      "Recall_micro performance: 1.0\n",
      "F1_micro performance: 1.0\n",
      "\n",
      "Max score obtained for: 14 with value: 0.8857132563543675\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(random_state=0)\n",
    "print_everything(X3, y3, model)\n",
    "get_best_hyperparam(2,0,X3,y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd794862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  [1. 1. 1. 1. 1.]\n",
      "precision_micro:  [1. 1. 1. 1. 1.]\n",
      "recall_micro:  [1. 1. 1. 1. 1.]\n",
      "f1_micro:  [1. 1. 1. 1. 1.]\n",
      "\n",
      "Mean for train scores:  1.0\n",
      "Mean for test scores:  1.0\n",
      "\n",
      "Accuracy performance: 1.0\n",
      "Precision_micro performance: 1.0\n",
      "Recall_micro performance: 1.0\n",
      "F1_micro performance: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(X3, y3)\n",
    "print_everything(X3, y3, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f79864b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  [0.44530494 0.44433688 0.40658277 0.35465116 0.29554264]\n",
      "precision_micro:  [0.44530494 0.44433688 0.40658277 0.35465116 0.29554264]\n",
      "recall_micro:  [0.44530494 0.44433688 0.40658277 0.35465116 0.29554264]\n",
      "f1_micro:  [0.44530494 0.44433688 0.40658277 0.35465116 0.29554264]\n",
      "\n",
      "Mean for train scores:  0.40679513487755425\n",
      "Mean for test scores:  0.38928367740531455\n",
      "\n",
      "Accuracy performance: 0.38928367740531455\n",
      "Precision_micro performance: 0.38928367740531455\n",
      "Recall_micro performance: 0.38928367740531455\n",
      "F1_micro performance: 0.38928367740531455\n",
      "\n",
      "Max score obtained for: 14 with value: 0.8857132563543675\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "rfc.fit(X3, y3)\n",
    "print_everything(X3,y3,rfc)\n",
    "get_best_hyperparam(3,2,X3,y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ca61760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  [0.59244918 0.61374637 0.61277832 0.64922481 0.36337209]\n",
      "precision_micro:  [0.59244918 0.61374637 0.61277832 0.64922481 0.36337209]\n",
      "recall_micro:  [0.59244918 0.61374637 0.61277832 0.64922481 0.36337209]\n",
      "f1_micro:  [0.59244918 0.61374637 0.61277832 0.64922481 0.36337209]\n",
      "\n",
      "Mean for train scores:  0.5940339240948523\n",
      "Mean for test scores:  0.5663141523522217\n",
      "\n",
      "Accuracy performance: 0.5663141523522217\n",
      "Precision_micro performance: 0.5663141523522217\n",
      "Recall_micro performance: 0.5663141523522217\n",
      "F1_micro performance: 0.5663141523522217\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svc = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "svc.fit(X3, y3)\n",
    "print_everything(X3,y3,svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c939f653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  [1. 1. 1. 1. 1.]\n",
      "precision_micro:  [1. 1. 1. 1. 1.]\n",
      "recall_micro:  [1. 1. 1. 1. 1.]\n",
      "f1_micro:  [1. 1. 1. 1. 1.]\n",
      "\n",
      "Mean for train scores:  1.0\n",
      "Mean for test scores:  1.0\n",
      "\n",
      "Accuracy performance: 1.0\n",
      "Precision_micro performance: 1.0\n",
      "Recall_micro performance: 1.0\n",
      "F1_micro performance: 1.0\n",
      "\n",
      "Max score obtained for: 4 with value: 1.0\n"
     ]
    }
   ],
   "source": [
    "dataset4 = './data/Years.txt'\n",
    "data4 = pd.read_csv(dataset4, delimiter = ',')\n",
    "\n",
    "X4 = data4.values[:, 1:]\n",
    "y4 = data4.values[:, 1]\n",
    "y4 = y4.astype('int')\n",
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "print_everything(X4, y4, model)\n",
    "get_best_hyperparam(1,5,X4,y4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f15f3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  [1. 1. 1. 1. 1.]\n",
      "precision_micro:  [1. 1. 1. 1. 1.]\n",
      "recall_micro:  [1. 1. 1. 1. 1.]\n",
      "f1_micro:  [1. 1. 1. 1. 1.]\n",
      "\n",
      "Mean for train scores:  1.0\n",
      "Mean for test scores:  1.0\n",
      "\n",
      "Accuracy performance: 1.0\n",
      "Precision_micro performance: 1.0\n",
      "Recall_micro performance: 1.0\n",
      "F1_micro performance: 1.0\n",
      "\n",
      "Max score obtained for: 4 with value: 1.0\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(random_state=0)\n",
    "print_everything(X4, y4, model)\n",
    "get_best_hyperparam(2,0,X4,y4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d31427a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  [1. 1. 1. 1. 1.]\n",
      "precision_micro:  [1. 1. 1. 1. 1.]\n",
      "recall_micro:  [1. 1. 1. 1. 1.]\n",
      "f1_micro:  [1. 1. 1. 1. 1.]\n",
      "\n",
      "Mean for train scores:  1.0\n",
      "Mean for test scores:  1.0\n",
      "\n",
      "Accuracy performance: 1.0\n",
      "Precision_micro performance: 1.0\n",
      "Recall_micro performance: 1.0\n",
      "F1_micro performance: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(X4, y4)\n",
    "print_everything(X4, y4, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aea6b1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  [0.93442623 0.93442623 0.95081967 0.93333333 0.93333333]\n",
      "precision_micro:  [0.93442623 0.93442623 0.95081967 0.93333333 0.93333333]\n",
      "recall_micro:  [0.93442623 0.93442623 0.95081967 0.93333333 0.93333333]\n",
      "f1_micro:  [0.93442623 0.93442623 0.95081967 0.93333333 0.93333333]\n",
      "\n",
      "Mean for train scores:  0.9372921130496887\n",
      "Mean for test scores:  0.9372677595628416\n",
      "\n",
      "Accuracy performance: 0.9372677595628416\n",
      "Precision_micro performance: 0.9372677595628416\n",
      "Recall_micro performance: 0.9372677595628416\n",
      "F1_micro performance: 0.9372677595628416\n",
      "\n",
      "Max score obtained for: 4 with value: 1.0\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "rfc.fit(X4, y4)\n",
    "print_everything(X4,y4,rfc)\n",
    "get_best_hyperparam(3,2,X4,y4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a0b1442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  [1. 1. 1. 1. 1.]\n",
      "precision_micro:  [1. 1. 1. 1. 1.]\n",
      "recall_micro:  [1. 1. 1. 1. 1.]\n",
      "f1_micro:  [1. 1. 1. 1. 1.]\n",
      "\n",
      "Mean for train scores:  1.0\n",
      "Mean for test scores:  1.0\n",
      "\n",
      "Accuracy performance: 1.0\n",
      "Precision_micro performance: 1.0\n",
      "Recall_micro performance: 1.0\n",
      "F1_micro performance: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svc = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "svc.fit(X4, y4)\n",
    "print_everything(X4,y4,svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed4abcd",
   "metadata": {},
   "source": [
    "4. (number of models * 4 points = 20 points) Document in jupyter notebook each of the models used. If the same algorithm is used for more than one dataset, you can make a separate section with documentation of the algorithms + reference to the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a090d20a",
   "metadata": {},
   "source": [
    "# Classification methods used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193f7753",
   "metadata": {},
   "source": [
    "1. [KNN](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier)\n",
    "2. [Decision Tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)\n",
    "3. [Gaussian Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB)\n",
    "4. [SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC)\n",
    "5. [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91adbcb",
   "metadata": {},
   "source": [
    "## 1. KNN - (k-nearest neighbors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9093cb8e",
   "metadata": {},
   "source": [
    "In statistics, the K Nearest Neighbours (k-NN) algorithm is a non-parametric classification method. It is used for classification and regression. In both cases, the input consists of the K closest training examples in the data set. The result depends on whether k-NN is used for classification or regression:\n",
    "\n",
    "   In k-NN classification, the output is a class member. An object is classified by voting on the plurality of its neighbors, the object is assigned to the most common class among its k nearest neighbors (k is a positive integer, usually small). If k = 1, then the nearest neighbour is chosen.\n",
    "    \n",
    "   In the k-NN regression, the output is the value of the property of the object. This value is the average of the values of the k nearest neighbours."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e283f64",
   "metadata": {},
   "source": [
    "## 2. Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c1d06a",
   "metadata": {},
   "source": [
    "A decision tree is a decision support tool that uses a tree-like decision model and its possible consequences, including the outcomes of chance events, resource costs and utility. It is a way of displaying an algorithm that contains only conditional control instructions.\n",
    "\n",
    "Decision trees are commonly used in operations research, particularly in decision analysis, to help identify a strategy most likely to achieve a goal, but are also a popular tool in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a0805a",
   "metadata": {},
   "source": [
    "## 3. Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6971e1",
   "metadata": {},
   "source": [
    "In statistics, Naivi Bayes classifiers are a family of simple \"probabilistic classifiers\" based on the application of Bayes' theorem with strong assumptions of independence between features. They are among the simplest Bayesian network models, but combined with kernel density estimation, they can achieve higher levels of accuracy.\n",
    "\n",
    "Naivi Bayes classifiers are highly scalable, requiring a number of linear parameters in the number of variables (features/predictors) in a learning problem. Training with maximum likelihood can be done by evaluating a closed-form expression, which requires linear time, rather than by costly iterative approximation, as is used for many other types of classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead04f54",
   "metadata": {},
   "source": [
    "## 4. SVM - (Support-vector machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b984bb58",
   "metadata": {},
   "source": [
    "In machine learning, support vector machines (SVMs) are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis.\n",
    "\n",
    "Given a set of training examples, each marker belongs to a category. An SVM training algorithm builds a model that assigns new examples to one category or another, making it a non-probabilistic binary linear classifier. An SVM maps the training examples to points in space so as to maximize the width of the gap between the two categories. New examples are then mapped to the same space and predicted to belong to a category based on which side of the gap they fall on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976afb8a",
   "metadata": {},
   "source": [
    "## 5. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0630d845",
   "metadata": {},
   "source": [
    "Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that works by building a multitude of decision trees during training and outputting the class that is the class mode (classification) or mean/average prediction ( regression) of the individual trees.\n",
    "\n",
    "Random decision trees correct the habit of decision trees to match their training set. Random forests generally outperform decision trees, but their accuracy is lower than gradient trees. However, data characteristics can affect their performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
