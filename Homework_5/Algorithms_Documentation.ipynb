{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ccd4a91",
   "metadata": {},
   "source": [
    "# Regression models used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20fafe1",
   "metadata": {},
   "source": [
    "1. [Linear Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "2. [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)\n",
    "3. [Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)\n",
    "4. [Elastic Net](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html)\n",
    "5. [Bayesian Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.BayesianRidge.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ad17ac",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efa1bce",
   "metadata": {},
   "source": [
    "In statistics, linear regression is a linear approach to modeling the relationship between a scalar response and one or more explanatory variables (also known as dependent and independent variables). The case of an explanatory variable is called simple linear regression; for more than one, the process is called multiple linear regression. This term is distinct from multivariate linear regression, where multiple correlated dependent variables are predicted rather than a single scalar variable.\r\n",
    "\r\n",
    "In linear regression, relationships are modeled using linear predictive functions whose unknown model parameters are estimated from the data. Such models are called linear models. Most commonly, the conditional mean of the response given by the values of the explanatory (or predictor) variables is assumed to be an affine function of these values; less often, the conditional median or other quantile is used. Like all forms of regression analysis, linear regression focuses on the conditional probability distribution of the response given the values of the predictors, rather than the joint probability distribution of all these variables, which is the domain of multivariate analysis.\r\n",
    "\r\n",
    "Linear regression was the first type of regression analysis to be rigorously studied and widely used in practical applications. This is because models that depend linearly on their unknown parameters are easier to fit than models that are nonlinearly related to their parameters, and because the statistical properties of the resulting estimators are easier to determine.\r\n",
    "\r\n",
    "Linear regression has many practical uses. Most apps fall into one of the following two broad categories:\r\n",
    "\r\n",
    "If the goal is prediction, forecasting, or error reduction, linear regression can be used to fit a predictive model to an observed data set of response values and explanatory variables. After developing such a model, if additional values of the explanatory variables are collected without an accompanying response value, the fitted model can be used to make a prediction of the response.\r\n",
    "\r\n",
    "If the goal is to explain the variation in the response variable that can be attributed to variation in the explanatory variables, linear regression analysis can be applied to quantify the strength of the relationship between the response and the explanatory variables and, in particular, to determine whether some explanatory variables may not have not at all a linear relationship with the response or to identify which subsets of explanatory variables may contain redundant information about the response.\r\n",
    "\r\n",
    "Linear regression models are often fitted using the least-squares approach, but can also be fitted in other ways, such as by minimizing \"lack of fit\" in other norms (such as least absolute deviation regression) or by minimizing a penalty version of the least squares cost function as in crest (L2-norm penalty) and lasso (L1-norm penalty) regression. Instead, the least squares approach can be used to fit models that are not linear models. Thus, although the terms \"least squares\" and \"linear model\" are closely related, they are not synonymous.\r\n",
    "\r\n",
    "**Bibliography:** <br>\r\n",
    "     https://en.wikipedia.org/wiki/Linear_regression <br>\r\n",
    "     https://machinelearningmastery.com/linear-regression-for-machine-learning/\r\n",
    "    \r\n",
    "  ![LinearRegression](./images/LinearRegression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151a2b3e",
   "metadata": {},
   "source": [
    "## Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dab8eb3",
   "metadata": {},
   "source": [
    "Ridge regression is a method for estimating the coefficients of multiple regression models in scenarios where the independent variables are highly correlated. It has uses in fields including econometrics, chemistry, and engineering.\r\n",
    "\r\n",
    "The theory was first introduced by Hoerl and Kennard in 1970 in their technometric papers \"RIDGE Regressions: Biased Estimation of Nonorthogonal Problems\" and \"RIDGE Regressions: Applications to Nonorthogonal Problems\". This was the result of ten years of research in the field of ridge analysis.\r\n",
    "\r\n",
    "Ridge regression was developed as a possible solution to the imprecision of least square estimators when linear regression models have some multicollinear (highly correlated) independent variables - by creating a ridge regression (RR) estimator. This provides a more accurate estimate of the ridge parameters because the variance and the mean square estimator are often smaller than the smallest previously derived estimators.\r\n",
    "\r\n",
    "For any type of regression machine learning models, the usual regression equation forms the basis which is written as:\r\n",
    "\r\n",
    "Y = XB + e\r\n",
    "\r\n",
    "Where Y is the dependent variable, X represents the independent variables, B is the regression coefficients to be evaluated, and e represents the errors are residuals.\r\n",
    "\r\n",
    "Once we add the lambda function to this equation, it accounts for variance that is not accounted for by the overall model. Once the data is ready and identified to be part of L2 regularization, there are steps you can take.\r\n",
    "\r\n",
    "In ridge regression, the first step is to standardize the variables (both dependent and independent) by subtracting their means and dividing by their standard deviations. This causes a notational challenge because we must somehow indicate whether the variables in a given formula are standardized or not. Regarding standardization, all ridge regression calculations are based on standardized variables. When the final regression coefficients are displayed, they are scaled back to their original scale. However, crest marks are on a standardized scale.\r\n",
    "\r\n",
    "**Bibliography:** <br>\r\n",
    "     https://en.wikipedia.org/wiki/Ridge_regression <br>\r\n",
    "     https://towardsdatascience.com/ridge-regression-for-better-usage-2f19b3a202db\r\n",
    "    \r\n",
    "![Ridge](./images/Ridge.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef8c115",
   "metadata": {},
   "source": [
    "## Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc7381f",
   "metadata": {},
   "source": [
    "In statistics and machine learning, the Lasso (least absolute contraction and selection operator; also Lasso or LASSO) is a regression analysis method that performs both variable selection and regularization to increase model prediction accuracy and interpretability statistical result. It was originally introduced in geophysics, and later by Robert Tibshirani, who coined the term.\r\n",
    "\r\n",
    "Lasso was originally formulated for linear regression models. This simple case reveals a substantial amount about the estimator. These include its relation to ridge regression and best subset selection, and connections between lasso coefficient estimates and so-called soft thresholding. It also shows that (like standard linear regression) the coefficient estimates need not be unique if the covariates are collinear.\r\n",
    "\r\n",
    "Although originally defined for linear regression, lasso regularization is easily extended to other statistical models, including generalized linear models, generalized estimating equations, proportional hazards models, and M-estimators. Lasso's ability to perform subset selection is based on the form of the constraint and has a variety of interpretations, including geometry, Bayesian statistics, and convex analysis.\r\n",
    "\r\n",
    "LASSO is closely related to basic tracking denoising.\r\n",
    "\r\n",
    "Lasso regression performs L1 regularization, which adds a penalty equal to the absolute value of the magnitude of the coefficients. This type of regularization can lead to sparse models with few coefficients; Some coefficients may become zero and removed from the model. Larger penalties result in coefficient values closer to zero, which is ideal for producing simpler models. On the other hand, L2 regularization (eg Ridge regression) does not result in the removal of sparse coefficients or patterns. This makes Lasso much easier to play than Ridge. Lasso solutions are quadratic programming problems that are best solved with software.\r\n",
    "\r\n",
    "A problem with linear regression is that the estimated model coefficients can become large, making the model sensitive to inputs and possibly unstable. This is especially true for problems with few observations (samples) or more samples (n) than input predictors (p) or variables (so-called p >> n problems).\r\n",
    "\r\n",
    "One approach to addressing the stability of regression models is to change the loss function to include additional costs for a model that has large coefficients. Linear regression models that use these modified loss functions during training are collectively called penalized linear regression.\r\n",
    "\r\n",
    "A popular penalty is to penalize a model based on the sum of absolute coefficient values. This is called the L1 penalty. An L1 penalty minimizes the size of all coefficients and allows some coefficients to be reduced to zero, which removes the predictor from the model.\r\n",
    "\r\n",
    "l1_penalty = sum j = 0 to p abs (beta_j)\r\n",
    "An L1 penalty minimizes the size of all coefficients and allows any coefficient to go to zero, effectively removing input features from the model.\r\n",
    "\r\n",
    "Before Lasso, the most widely used method for choosing covariates was stepwise selection. This approach improves prediction accuracy only in certain cases, such as when only a few covariates have a strong relationship with the outcome. However, in other cases, it may increase the prediction error.\r\n",
    "\r\n",
    "**Bibliography:** <br>\r\n",
    "https://en.wikipedia.org/wiki/Lasso_(statistics) <br>\r\n",
    "https://www.statisticshowto.com/lasso-regression/\r\n",
    "\r\n",
    "![Lasso](./images/Lasso.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7367ee4c",
   "metadata": {},
   "source": [
    "## Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f351d16a",
   "metadata": {},
   "source": [
    "Elastic Net is a popular type of regularized linear regression that combines two popular penalties, specifically the L1 and L2 penalty function.\r\n",
    "\r\n",
    "Linear regression refers to a model that assumes a linear relationship between the input variables and the target variable.\r\n",
    "\r\n",
    "With a single input variable, this relationship is a line, and with larger dimensions, this relationship can be thought of as a hyperplane connecting the input variables to the target variable. The model coefficients are found through an optimization process that seeks to minimize the sum of the squared error between the predictions (yhat) and the expected target values (y).\r\n",
    "\r\n",
    "loss = sum i = 0 to n (y_i - yhat_i) ^ 2 <br>\r\n",
    "A problem with linear regression is that the estimated model coefficients can become large, making the model sensitive to inputs and possibly unstable. This is especially true for problems with few observations (samples) or more samples (n) than input predictors (p) or variables (so-called p >> n problems).\r\n",
    "\r\n",
    "One approach to addressing the stability of regression models is to change the loss function to include additional costs for a model that has large coefficients. Linear regression models that use these modified loss functions during training are collectively called penalized linear regression.\r\n",
    "\r\n",
    "A popular penalty is to penalize a model based on the sum of squared coefficient values. This is called the L2 penalty. An L2 penalty minimizes the size of all coefficients, although it prevents any coefficients from being dropped from the model.\r\n",
    "\r\n",
    "l2_penalty = sum j = 0 to p beta_j ^ 2 <br>\r\n",
    "Another popular penalty is to penalize a model based on the sum of absolute coefficient values. This is called the L1 penalty. An L1 penalty minimizes the size of all coefficients and allows some coefficients to be reduced to zero, which removes the predictor from the model.\r\n",
    "\r\n",
    "l1_penalty = sum j = 0 to p abs (beta_j) <br>\r\n",
    "Elastic Net is a penalized linear regression model that includes both L1 and L2 penalties during training.\r\n",
    "\r\n",
    "Using the terminology from \"Elements of Statistical Learning\", a hyperparameter \"alpha\" is provided to assign how much weight is given to each of the L1 and L2 penalties. Alpha is a value between 0 and 1 and is used to weight the contribution of the L1 penalty and a minus alpha value is used to weight the L2 penalty.\r\n",
    "\r\n",
    "elastic_net_penalty = (alpha * l1_penalty) + ((1 - alpha) * l2_penalty) <br>\r\n",
    "For example, an alpha of 0.5 would give a 50% contribution of each penalty to the loss function. An alpha value of 0 gives all the weight of the L2 penalty and a value of 1 gives all the weight of the L1 penalty.\r\n",
    "\r\n",
    "The alpha parameter determines the penalty mix and is often preselected for qualitative reasons.\r\n",
    "\r\n",
    "The advantage is that the elastic network allows for a balance of both penalties, which can lead to better performance than a model with one or the other penalty on certain problems.\r\n",
    "\r\n",
    "Another hyperparameter called \"lambda\" is provided which controls the weighting of the sum of both penalties to the loss function. A default value of 1.0 is used to use the fully weighted penalty; a value of 0 excludes the penalty. Very small values of lambda, such as 1e-3 or less, are common.\r\n",
    "\r\n",
    "elastic_net_loss = loss + (lambda * elastic_net_penalty)\r\n",
    "\r\n",
    "**Bibliography:** <br>\r\n",
    "https://en.wikipedia.org/wiki/Elastic_net_regularization <br>\r\n",
    "https://machinelearningmastery.com/elastic-net-regression-in-python/\r\n",
    "\r\n",
    "![Elastic Net](./images/ElasticNet.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3588b348",
   "metadata": {},
   "source": [
    "## Bayesian Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f4d305",
   "metadata": {},
   "source": [
    "In statistics, Bayesian linear regression is an approach to linear regression in which statistical analysis is performed in the context of Bayesian inference. When the regression model has errors that are normally distributed, and if some form of prior distribution is assumed, explicit results are available for the posterior probability distributions of the model parameters.\r\n",
    "\r\n",
    "The output, is generated from a normal (Gaussian) distribution characterized by a mean and a variance. The mean for linear regression is the transpose of the weight matrix multiplied by the predictor matrix. The variance is the square of the standard deviation σ (multiplied by the identity matrix, since this is a multidimensional formulation of the model).\r\n",
    "The purpose of Bayesian Linear Regression is not to find the \"best\" value of the model parameters, but rather to determine the posterior distribution for the model parameters. Not only is the response generated from a probability distribution, but the model parameters are also assumed to come from a distribution. The posterior probability of the model parameters is conditional on the training inputs and outputs:\r\n",
    "\r\n",
    "Unlike OLS, we have a posterior distribution for the model parameters that is proportional to the probability of the data multiplied by the prior probability of the parameters. Here we can see the two main benefits of Bayesian linear regression. br\r\n",
    "\r\n",
    "Priors: If we have domain knowledge or guesses about what the model parameters should be, we can include them in our model, unlike the frequentist approach which assumes that everything we need to know about the parameters comes from the data. If we do not have prior estimates, we can use non-informative priors for the parameters, such as a normal distribution. br\r\n",
    "\r\n",
    "Posterior: The result of performing Bayesian Linear Regression is a distribution of possible model parameters based on the data and the prior. This allows us to quantify our uncertainty about the model: if we have fewer data points, the posterior distribution will be more spread out.\r\n",
    "As the amount of data points increases, the likelihood removes the prior, and in the case of infinite data, the parameter outputs converge to the values obtained from OLS.\r\n",
    "Formulating the model parameters as distributions encapsulates the Bayesian worldview: we start with an initial estimate, our prior, and as we gather more evidence, our model becomes less wrong. Bayesian reasoning is a natural extension of our intuition. Often, we have an initial hypothesis, and as we collect data that either supports or disproves our ideas, we change our world model (ideally so we would argue)!\r\n",
    "\r\n",
    "The basic procedure for implementing Bayesian linear regression is: specify priors for the model parameters (we used normal distributions in this example), create a model that maps the training inputs to the training outputs, and then have a Markov Chain Monte Carlo (MCMC) algorithm draw samples from the posterior distribution for the model parameters. The end result will be posterior distributions for the parameters. We can inspect these distributions to get an idea of what is going on.\r\n",
    "\r\n",
    "**Bibliography:** <br>\r\n",
    "https://en.wikipedia.org/wiki/Bayesian_linear_regression <br>\r\n",
    "https://towardsdatascience.com/introduction-to-bayesian-linear-regression-e66e60791ea7\r\n",
    "\r\n",
    "![Bayesian Ridge](images/BayesianRidge.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc400f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
